"""
Stage2 SENTENCE Node - Handles SENTENCE type questions
"""
import os
import json
import pandas as pd
from datetime import datetime
from typing import Dict, Any, Optional
from io_layer.llm.client import LLMClient
from io_layer.embedding import VectorEmbedding
from .prep_sentence import get_column_locations, extract_question_choices
from utils.project_manager import get_project_manager
from config.config import settings
from config.prompt.prompt_loader import load_prompt_config, resolve_branch
from utils.pydantic_utils import extract_llm_response_data


def stage2_sentence_node(state: Dict[str, Any], deps: Optional[Any] = None) -> Dict[str, Any]:
    """
    SENTENCE íƒ€ì… ì§ˆë¬¸ ì²˜ë¦¬ ë…¸ë“œ
    
    SENTENCE íƒ€ì…ì€ depend, depend_pos_neg, pos_neg ë“±ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê°€ í•„ìš”
    ë‘ ê°œì˜ LLMì„ ì‚¬ìš©: 1) ë¬¸ë²• êµì • (gpt-4.1), 2) ë¬¸ì¥ ë¶„ì„ (gpt-4.1-nano)
    
    Args:
        state: Current graph state
        deps: Dependencies (contains llm_client)
        
    Returns:
        Updated state with sentence processing results and CSV file path
    """
    current_question_id = state.get('current_question_id')
    current_question_type = state.get('current_question_type')
    
    print(f"stage2_sentence_node: Processing SENTENCE type question {current_question_id} ({current_question_type})")
    
    # VectorEmbedding ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    embed = VectorEmbedding()
    
    try:
        # Configì—ì„œ prompt ë¡œë“œ
        prompt_config = load_prompt_config()
        grammar_branch = resolve_branch(prompt_config, "sentence_grammar_check")
        
        # Question typeì— ë”°ë¥¸ analysis branch ì„ íƒ
        analysis_branch_key = f"sentence_{current_question_type}_split"
        analysis_branch = resolve_branch(prompt_config, analysis_branch_key)
        
        # Fallback: ê¸°ë³¸ sentence_only branch ì‚¬ìš©
        if not analysis_branch:
            analysis_branch = resolve_branch(prompt_config, "sentence_only")
            
        if not grammar_branch or not analysis_branch:
            print(f"Warning: Missing prompt config for {current_question_type}")
            # Fallback to hardcoded prompts (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
            grammar_branch = None
            analysis_branch = None
        
        # ë‘ ê°œì˜ LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (stage2_prompt_work.py íŒ¨í„´ ë”°ë¼)
        llm_client_large = LLMClient(model_key="gpt-4.1")     # ë¬¸ë²• êµì •ìš©
        llm_client_nano = LLMClient(model_key="gpt-4.1-nano") # ë¬¸ì¥ ë¶„ì„ìš©
        
        # ì»¬ëŸ¼ ë°ì´í„° ì¶”ì¶œ
        if current_question_type in ["depend", "depend_pos_neg"]:
            depend_df, text_df = get_column_locations(state, current_question_id, "SENTENCE")
            print(f"Found depend data: {len(depend_df)} rows, text data: {len(text_df)} rows")
        else:
            result = get_column_locations(state, current_question_id, "SENTENCE")
            if isinstance(result, tuple):
                text_df = result[1] if len(result) > 1 else result[0]
            else:
                text_df = result
            depend_df = None
            print(f"Found text data: {len(text_df)} rows")
        
        # ì§ˆë¬¸ ì •ë³´ ì¶”ì¶œ
        matched_questions = state.get('matched_questions', {})
        
        # matched_questionsê°€ listì¸ ê²½ìš° dictë¡œ ë³€í™˜
        if isinstance(matched_questions, list):
            matched_questions_dict = {}
            for item in matched_questions:
                if isinstance(item, dict) and 'question_id' in item:
                    matched_questions_dict[item['question_id']] = {'question_info': item}
            matched_questions = matched_questions_dict
        
        question_info = matched_questions[current_question_id]['question_info']
        survey_context = state.get('survey_context', '')
        question_summary = question_info.get('question_summary', question_info.get('question_text', ''))
        
        # ì‘ë‹µ ì‚¬ì „ êµ¬ì„± (depend íƒ€ì…ì¸ ê²½ìš°)
        response_dict = {}
        if current_question_type in ["depend", "depend_pos_neg"] and depend_df is not None:
            unique_depends = depend_df.iloc[:, 0].dropna().unique().tolist()
            unique_depends = [str(int(x)) for x in unique_depends if pd.notna(x)]
            
            try:
                response_mapping = extract_question_choices(llm_client_nano, question_summary, unique_depends)
                
                # response_mappingì´ dictì¸ì§€ í™•ì¸í•˜ê³  answers í‚¤ê°€ ìˆëŠ”ì§€ ì²´í¬
                if isinstance(response_mapping, dict) and 'answers' in response_mapping:
                    response_dict = response_mapping['answers']
                    if not isinstance(response_dict, dict):
                        print(f"Warning: response_mapping['answers'] is not a dict, type: {type(response_dict)}")
                        response_dict = {}
                else:
                    print(f"Warning: response_mapping is not valid, type: {type(response_mapping)}")
                    response_dict = {}
            except Exception as e:
                print(f"Error extracting question choices: {e}")
                response_dict = {}
        
        # ë‘ ë‹¨ê³„ LLM ì²˜ë¦¬
        result_data = []
        total_price = 0
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œë§Œ ì²˜ë¦¬
        # test_limit = min(10, len(text_df))
        test_limit = len(text_df)
        # print(f"ğŸ§ª TEST MODE: Processing only {test_limit} rows (out of {len(text_df)})")
        
        for i in range( len(text_df)):
            # ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            row_texts = text_df.iloc[i].dropna().astype(str).tolist()
            if not row_texts:
                continue
            original_text = row_texts[0].strip()
            
            # 1ë‹¨ê³„: ë¬¸ë²• êµì • (llm_client_large)
            if grammar_branch:
                # Configì—ì„œ ë¡œë“œëœ prompt ì‚¬ìš©
                grammar_prompt = grammar_branch['user_template'].format(
                    survey_context=survey_context,
                    answer=original_text
                )
                grammar_system = grammar_branch['system']
            else:
                # Fallback: ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ prompt
                grammar_prompt = f"""
summary of the survey: {survey_context}
answer: {original_text}
"""
                grammar_system = """Correct grammar and typos in Korean survey responses.

- Correct ONLY the answer, making it natural Korean.  
- Use the question only as context if needed.  

Return result as valid JSON in the format:  
{ "corrected": "<corrected answer>"}  

Rules:  
- Do not change the meaning.  
- Correct typos and unnatural expressions into the most natural and common form in Korean survey responses.  
- When multiple corrections are possible, prefer the one that best fits everyday consumer feedback context.  
- Do not output anything except the JSON."""
            
            grammar_response = llm_client_large.chat(
                system=grammar_system,
                user=grammar_prompt,
                schema=grammar_branch.get('schema') if grammar_branch else None
            )
            
            # Schema ê¸°ë°˜ ì‘ë‹µ ì²˜ë¦¬
            if grammar_branch and grammar_branch.get('schema'):
                data = extract_llm_response_data(grammar_response[0])
                corrected_text = data.get('corrected', original_text)
            else:
                # Fallback: JSON íŒŒì‹±
                try:
                    corrected_data = json.loads(grammar_response[0])
                    corrected_text = corrected_data['corrected']
                except json.JSONDecodeError:
                    corrected_text = original_text
                
            # ë¹„ìš© ê³„ì‚°
            if hasattr(grammar_response[1], 'cost_usd'):
                total_price += grammar_response[1].cost_usd
            elif hasattr(grammar_response[1], 'total_cost'):
                total_price += grammar_response[1].total_cost
            
            # 2ë‹¨ê³„: ë¬¸ì¥ ë¶„ì„ (llm_client_nano)
            if analysis_branch:
                # Configì—ì„œ ë¡œë“œëœ prompt ì‚¬ìš©
                if current_question_type in ["depend", "depend_pos_neg"] and depend_df is not None:
                    depend_value = str(int(depend_df.iloc[i, 0]))
                    # response_dictê°€ dictì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if isinstance(response_dict, dict):
                        sub_explanation = response_dict.get(depend_value, "")
                    else:
                        sub_explanation = ""
                        print(f"Warning: response_dict is not a dictionary, type: {type(response_dict)}")
                    
                    analysis_prompt = analysis_branch['user_template'].format(
                        survey_context=survey_context,
                        question_summary=question_summary,
                        sub_explanation=sub_explanation,
                        corrected_answer=corrected_text
                    )
                else:
                    analysis_prompt = analysis_branch['user_template'].format(
                        survey_context=survey_context,
                        question_summary=question_summary,
                        corrected_answer=corrected_text
                    )
                
                analysis_system = analysis_branch['system']
            else:
                # Fallback: ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ prompt
                if current_question_type in ["depend", "depend_pos_neg"] and depend_df is not None:
                    depend_value = str(int(depend_df.iloc[i, 0]))
                    # response_dictê°€ dictì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if isinstance(response_dict, dict):
                        sub_explanation = response_dict.get(depend_value, "")
                    else:
                        sub_explanation = ""
                        print(f"Warning: response_dict is not a dictionary, type: {type(response_dict)}")
                    
                    analysis_prompt = f"""
question: {question_summary}
sub_explanation: {sub_explanation}
answer: {corrected_text}
"""
                else:
                    analysis_prompt = f"""
question: {question_summary}
answer: {corrected_text}
"""
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ íƒ€ì…ë³„)
                if current_question_type == "depend_pos_neg":
                    pos_neg_rule = "7. **Pos/Neg classification**: If the question is about sentiment (positive/negative), classify the answer accordingly with sub_explanation and set `\"pos.neg\"` to `\"POSITIVE\"`or `\"NEGATIVE\"`."
                elif current_question_type == "pos_neg":
                    pos_neg_rule = "7. **Pos/Neg classification**: If the question is about sentiment (positive/negative), classify the answer accordingly and set `\"pos.neg\"` to `\"POSITIVE\"`or `\"NEGATIVE\"`."
                else:
                    pos_neg_rule = "7. pos.neg is just placeholder. just return \"NEUTRAL\""
                
                analysis_system = f"""You are a survey response interpreter.

### Rules
1. **Nuance reference**: Context fields are only for reference. Do not include them in the final output.  
2. type-of-error (typo) detection: Identify and correct any typographical errors in the answer including spelling mistakes, grammatical errors, and incorrect word usage.  
3. **Question matching judgment**:  
   - Set `"matching_question": true` unless the answer is clearly irrelevant or meaningless.  
   - Even short or abstract answers like *"trustworthy"*, *"it feels reliable"*, *"good"*, *"satisfying"* should be treated as `true`.  
   - Set `"matching_question": false` only if the answer is irrelevant, empty, meaningless tokens, or pure emotional expression with no semantic relation (e.g., "Wow!", "I'm so excited").  
   - Ignore filler tokens such as `merged`, `nan`, `NaN`, `None`, `NULL`, or `Name: 0, dtype: object` before evaluation.  
4. **Atomic sentence split**: If the answer contains multiple semantic units, split it into at most 3 atomic sentences (Subjectâ€“Verbâ€“Complement based).  
5. **S/V/C keyword extraction**: For each atomic sentence, extract core keywords:  
   - S = subject (main entity)  
   - V = verb (main action/state) format in "-ë‹¤" form
   - C = complement/object (if any)  
   - Only extract essential words, not particles or function words.  
6. **Output format**: Return the result format strictly in the following JSON format (schema stays in Korean):
{pos_neg_rule}

{{{{
  "matching_question": true/false,
  "pos.neg": "POSITIVE"/"NEGATIVE"/"NEUTRAL",
  "automic_sentence": ["ë¬¸ì¥1", "ë¬¸ì¥2", "ë¬¸ì¥3"],
  "SVC_keywords": {{{{
      "sentence1": {{{{
    "S": [],
    "V": [],
    "C": []}}}},
      "sentence2": {{{{
    "S": [],
    "V": [],
    "C": []}}}},
      "sentence3": {{{{
    "S": [],
    "V": [],
    "C": []
  }}}}
}}}}"""
            
            analysis_response = llm_client_nano.chat(
                system=analysis_system,
                user=analysis_prompt,
                schema=analysis_branch.get('schema') if analysis_branch else None
            )
            
            # Schema ê¸°ë°˜ ì‘ë‹µ ì²˜ë¦¬
            if analysis_branch and analysis_branch.get('schema'):
                data = extract_llm_response_data(analysis_response[0])
            else:
                # Fallback: JSON íŒŒì‹±
                try:
                    data = json.loads(analysis_response[0])
                except json.JSONDecodeError:
                    try:
                        data = json.loads(analysis_response[0].replace('```json', '').replace('```', ''))
                    except:
                        continue
            
            # dataê°€ listì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ê¸°ë³¸ dict êµ¬ì¡° ìƒì„±
            if isinstance(data, list):
                if len(data) > 0 and isinstance(data[0], dict):
                    data = data[0]
                else:
                    # ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                    data = {
                        'matching_question': False,
                        'pos.neg': 'NEUTRAL',
                        'automic_sentence': [],
                        'SVC_keywords': {}
                    }
            elif not isinstance(data, dict):
                # dataê°€ dictë„ listë„ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                data = {
                    'matching_question': False,
                    'pos.neg': 'NEUTRAL', 
                    'automic_sentence': [],
                    'SVC_keywords': {}
                }
                    
            # ë¹„ìš© ê³„ì‚°
            if hasattr(analysis_response[1], 'cost_usd'):
                total_price += analysis_response[1].cost_usd
            elif hasattr(analysis_response[1], 'total_cost'):
                total_price += analysis_response[1].total_cost
            
            # ë°ì´í„° ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
            if not isinstance(data, dict):
                data = {
                    'matching_question': False,
                    'pos_neg': 'NEUTRAL',
                    'automic_sentence': [],
                    'SVC_keywords': {}
                }
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            data.setdefault('matching_question', False)
            data.setdefault('pos_neg', 'NEUTRAL') 
            data.setdefault('automic_sentence', [])
            data.setdefault('SVC_keywords', {})
            
            # automic_sentenceê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if not isinstance(data['automic_sentence'], list):
                data['automic_sentence'] = []
                
            # SVC_keywordsê°€ dictì¸ì§€ í™•ì¸
            if not isinstance(data['SVC_keywords'], dict):
                data['SVC_keywords'] = {}
            
            # ê²°ê³¼ DataFrame í–‰ ìƒì„±
            automic_sentence = data['automic_sentence']
            svc_keywords = data['SVC_keywords']
            
            row_data = {
                'id': i,
                'pos.neg': data.get('pos_neg', 'NEUTRAL'),
                'matching_question': data.get('matching_question', False),
                'org_text': original_text,
                'correction_text': corrected_text,
                'org_text_embed': embed.embed(original_text) if original_text.strip() else [],
                'correction_text_embed': embed.embed(corrected_text) if corrected_text.strip() else [],
                'sentence_1': automic_sentence[0] if len(automic_sentence) > 0 else None,
                'sentence_2': automic_sentence[1] if len(automic_sentence) > 1 else None,
                'sentence_3': automic_sentence[2] if len(automic_sentence) > 2 else None,
                'sentence_1_embed': embed.embed(automic_sentence[0]) if len(automic_sentence) > 0 and automic_sentence[0] and str(automic_sentence[0]).strip() else [],
                'sentence_2_embed': embed.embed(automic_sentence[1]) if len(automic_sentence) > 1 and automic_sentence[1] and str(automic_sentence[1]).strip() else [],
                'sentence_3_embed': embed.embed(automic_sentence[2]) if len(automic_sentence) > 2 and automic_sentence[2] and str(automic_sentence[2]).strip() else [],
                'S_1': ', '.join(svc_keywords.get('sentence1', {}).get('S', [])),
                'V_1': ', '.join(svc_keywords.get('sentence1', {}).get('V', [])),
                'C_1': ', '.join(svc_keywords.get('sentence1', {}).get('C', [])),
                'S_2': ', '.join(svc_keywords.get('sentence2', {}).get('S', [])),
                'V_2': ', '.join(svc_keywords.get('sentence2', {}).get('V', [])),
                'C_2': ', '.join(svc_keywords.get('sentence2', {}).get('C', [])),
                'S_3': ', '.join(svc_keywords.get('sentence3', {}).get('S', [])),
                'V_3': ', '.join(svc_keywords.get('sentence3', {}).get('V', [])),
                'C_3': ', '.join(svc_keywords.get('sentence3', {}).get('C', []))
            }
            
            result_data.append(row_data)
            print(f"Processed {i+1}/{test_limit} responses, Total cost: ${total_price:.4f}")
        
        # DataFrame ìƒì„±
        result_df = pd.DataFrame(result_data)
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ë¥¼ í†µí•´ CSV íŒŒì¼ ê²½ë¡œ ìƒì„±
        project_name = state.get('project_name', 'unknown')
        project_manager = get_project_manager(project_name)
        
        csv_path = project_manager.get_stage2_csv_path(
            current_question_id, 
            current_question_type, 
            timestamp
        )
        
        result_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        print(f"SENTENCE processing completed for {current_question_id}")
        print(f"Results saved to: {csv_path}")
        print(f"Total processing cost: ${total_price:.4f}")
        print(f"Processed {len(result_df)} rows")
        
        # ê¸°ì¡´ matched_questionsì— ë°ì´í„° ê²½ë¡œ ì •ë³´ ì¶”ê°€
        matched_questions = state.get('matched_questions', {})
        if current_question_id in matched_questions:
            matched_questions[current_question_id]['stage2_data'] = {
                'csv_path': csv_path,
                'processing_type': 'SENTENCE',
                'rows_count': len(result_df),
                'timestamp': timestamp,
                'total_cost': total_price,
                'status': 'completed'
            }
        
        # ê²°ê³¼ ìƒíƒœ ë°˜í™˜
        current_total_cost = state.get('total_llm_cost_usd', 0.0)
        updated_total_cost = current_total_cost + total_price
        
        final_state = {
            **state,
            'stage2_sentence_processed': True,
            'stage2_processing_type': 'SENTENCE',
            'stage2_status': 'completed',
            'stage2_result_csv': csv_path,
            'stage2_processed_rows': len(result_df),
            'stage2_total_cost': total_price,
            'total_llm_cost_usd': updated_total_cost,  # ì „ì²´ LLM ë¹„ìš© ëˆ„ì  ì—…ë°ì´íŠ¸
            'matched_questions': matched_questions  # ì—…ë°ì´íŠ¸ëœ matched_questions
        }
        
        # Stage 2 SENTENCE ì²˜ë¦¬ í›„ state ì €ì¥
        project_name = state.get('project_name')
        if project_name and settings.SAVE_STATE_LOG:
            project_manager = get_project_manager(project_name)
            config = {'save_state_log': settings.SAVE_STATE_LOG}
            # current_stage ì—…ë°ì´íŠ¸ í›„ ì €ì¥
            final_state['current_stage'] = f'STAGE2_SENTENCE_{current_question_id}_COMPLETED'
            project_manager.save_state(dict(final_state), config)
        
        return final_state
        
    except Exception as e:
        print(f"Error in SENTENCE processing for {current_question_id}: {e}")
        return {
            **state,
            'stage2_error': f"SENTENCE processing failed: {str(e)}",
            'stage2_status': 'error',
            'stage2_processing_type': 'SENTENCE'
        }