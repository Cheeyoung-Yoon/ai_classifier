"""
Singleton-Aware Clustering System
MCLì˜ ì‹±ê¸€í†¤ ë³´ì¡´ ê°€ì¹˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ í´ëŸ¬ìŠ¤í„°ë§
NMI/ARI ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ í†µí•©
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, List, Tuple, Optional
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
import markov_clustering as mc
import warnings
warnings.filterwarnings('ignore')

# NMI/ARI í‰ê°€ ì‹œìŠ¤í…œ import
try:
    from .nmi_ari_evaluation import NMIARIEvaluator
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from nmi_ari_evaluation import NMIARIEvaluator

class SingletonAwareClustering:
    """
    ì‹±ê¸€í†¤ ì¸ì‹ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
    
    MCLì˜ ì² í•™(ì‹±ê¸€í†¤ ë³´ì¡´)ì„ ìœ ì§€í•˜ë©´ì„œ ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
    ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•
    
    NMI/ARI ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì¸¡ì •
    """
    
    def __init__(self, config: Optional[Dict] = None):
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {
            'mcl_max_time': 120,        # MCL ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            'mcl_inflation': 2.0,       # MCL inflation íŒŒë¼ë¯¸í„°
            'mcl_expansion': 2,         # MCL expansion íŒŒë¼ë¯¸í„°
            'mcl_similarity_threshold': 0.3,  # MCLìš© ìœ ì‚¬ë„ ì„ê³„ê°’
            
            'kmeans_max_clusters': 7,   # K-means ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜
            'kmeans_min_samples': 10,   # K-means ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            
            'dbscan_eps_range': (0.3, 0.8),     # DBSCAN eps ë²”ìœ„
            'dbscan_min_samples_ratio': 0.05,   # DBSCAN min_samples ë¹„ìœ¨
            
            'singleton_threshold': 0.15,         # ì‹±ê¸€í†¤ íŒì • ì„ê³„ê°’
            'quality_threshold': 0.2,            # NMI/ARI í’ˆì§ˆ ì„ê³„ê°’ (silhouette ëŒ€ì‹ )
            'min_cluster_size_ratio': 0.03,      # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„ìœ¨
        }
        
        # NMI/ARI í‰ê°€ì ì´ˆê¸°í™”
        self.evaluator = NMIARIEvaluator({
            'nmi_weight': 0.6,
            'ari_weight': 0.4,
            'min_cluster_size_ratio': self.config['min_cluster_size_ratio'],
            'max_clusters': self.config['kmeans_max_clusters'],
            'min_nmi_threshold': 0.1,
            'min_ari_threshold': 0.05
        })
        
        self.stats = {
            'method_used': None,
            'mcl_attempted': False,
            'mcl_success': False,
            'fallback_reason': None,
            'execution_time': 0,
            'n_clusters': 0,
            'n_singletons': 0,
            'evaluation_metrics': {}
        }
    
    def create_optimized_graph(self, embeddings: np.ndarray, k: int = 15) -> csr_matrix:
        """MCLì„ ìœ„í•œ ìµœì í™”ëœ ê·¸ë˜í”„ ìƒì„±"""
        
        # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)
        n = len(embeddings)
        
        # 2. KNN ê·¸ë˜í”„ ìƒì„± (ê° ë…¸ë“œë§ˆë‹¤ kê°œì˜ ìµœê·¼ì ‘ ì´ì›ƒ)
        knn_graph = np.zeros_like(similarity_matrix)
        
        for i in range(n):
            # ìê¸° ìì‹  ì œì™¸í•˜ê³  kê°œì˜ ìµœê·¼ì ‘ ì´ì›ƒ ì°¾ê¸°
            neighbors = np.argsort(similarity_matrix[i])[-k-1:-1]  # ìê¸° ìì‹  ì œì™¸
            for j in neighbors:
                knn_graph[i][j] = similarity_matrix[i][j]
        
        # 3. ëŒ€ì¹­í™” (undirected graph)
        knn_graph = (knn_graph + knn_graph.T) / 2
        
        # 4. ìµœì†Œ ì„ê³„ê°’ ì ìš© (ë„ˆë¬´ ì•½í•œ ì—°ê²° ì œê±°)
        threshold = np.percentile(knn_graph[knn_graph > 0], 50)  # ìƒìœ„ 50%ë§Œ ìœ ì§€
        knn_graph[knn_graph < threshold] = 0
        
        return csr_matrix(knn_graph)
    
    def try_mcl_clustering(self, embeddings: np.ndarray) -> Tuple[Optional[np.ndarray], Dict]:
        """ìµœì í™”ëœ MCL í´ëŸ¬ìŠ¤í„°ë§ ì‹œë„"""
        
        n_samples = len(embeddings)
        mcl_params = self.config['mcl_params']
        
        best_result = None
        best_score = -1
        best_info = {}
        
        print(f"   ğŸ”„ Trying MCL with optimized parameters...")
        
        for inflation in mcl_params['inflation_range']:
            for k in mcl_params['k_range']:
                for max_iters in mcl_params['max_iters']:
                    try:
                        # ìµœì í™”ëœ ê·¸ë˜í”„ ìƒì„±
                        graph = self.create_optimized_graph(embeddings, k=k)
                        
                        # MCL ì‹¤í–‰
                        result = mc.run_mcl(graph, inflation=inflation, iterations=max_iters)
                        clusters = mc.get_clusters(result)
                        
                        # í´ëŸ¬ìŠ¤í„° ë°°ì—´ë¡œ ë³€í™˜
                        labels = np.full(n_samples, -1)
                        for cluster_id, cluster_nodes in enumerate(clusters):
                            for node in cluster_nodes:
                                if node < n_samples:  # ë²”ìœ„ ì²´í¬
                                    labels[node] = cluster_id
                        
                        # í’ˆì§ˆ í‰ê°€
                        n_clusters = len(clusters)
                        singleton_count = sum(1 for cluster in clusters if len(cluster) == 1)
                        singleton_ratio = singleton_count / n_clusters if n_clusters > 0 else 0
                        
                        # ì œì•½ ì¡°ê±´ í™•ì¸
                        max_clusters = self.config['cluster_constraints']['max_per_column']
                        if n_clusters > max_clusters:
                            continue
                        
                        # Silhouette score ê³„ì‚° (í´ëŸ¬ìŠ¤í„°ê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ)
                        if n_clusters > 1:
                            try:
                                silhouette = silhouette_score(embeddings, labels)
                            except:
                                silhouette = 0
                        else:
                            silhouette = 0
                        
                        # ì ìˆ˜ ê³„ì‚° (singleton ê°ì§€ ë³´ë„ˆìŠ¤ í¬í•¨)
                        score = silhouette + (singleton_ratio * 0.1)  # singleton ë³´ë„ˆìŠ¤
                        
                        if score > best_score:
                            best_result = labels
                            best_score = score
                            best_info = {
                                'inflation': inflation,
                                'k': k,
                                'max_iters': max_iters,
                                'n_clusters': n_clusters,
                                'singleton_count': singleton_count,
                                'singleton_ratio': singleton_ratio,
                                'silhouette': silhouette,
                                'score': score
                            }
                        
                        print(f"     â€¢ inflation={inflation}, k={k}, iters={max_iters}")
                        print(f"       â†’ {n_clusters} clusters, {singleton_count} singletons, score={score:.3f}")
                        
                    except Exception as e:
                        continue
        
        if best_result is not None:
            print(f"   âœ… MCL succeeded: {best_info['n_clusters']} clusters, {best_info['singleton_count']} singletons")
            return best_result, best_info
        else:
            print(f"   âŒ MCL failed with all parameter combinations")
            return None, {}
    
    def limited_kmeans(self, embeddings: np.ndarray, max_k: int = 5) -> Tuple[np.ndarray, Dict]:
        """ì œí•œëœ K-means (singleton í›„ì²˜ë¦¬ í¬í•¨)"""
        
        n_samples = len(embeddings)
        min_cluster_size = int(n_samples * self.config['cluster_constraints']['min_cluster_size_ratio'])
        
        best_labels = None
        best_score = -1
        best_info = {}
        
        print(f"   ğŸ”„ Trying limited K-means (K=2-{max_k})...")
        
        for k in range(2, max_k + 1):
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(embeddings)
                
                # ì‘ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ singletonìœ¼ë¡œ ë³€í™˜
                cluster_counts = np.bincount(labels)
                small_clusters = np.where(cluster_counts < min_cluster_size)[0]
                
                # Singleton ë§ˆí‚¹ (-1ë¡œ ë³€ê²½)
                singleton_labels = labels.copy()
                for small_cluster in small_clusters:
                    singleton_labels[labels == small_cluster] = -1
                
                # ë‚˜ë¨¸ì§€ í´ëŸ¬ìŠ¤í„° ì¬ë„˜ë²„ë§
                unique_labels = np.unique(singleton_labels)
                unique_labels = unique_labels[unique_labels != -1]
                label_mapping = {old: new for new, old in enumerate(unique_labels)}
                
                for old_label, new_label in label_mapping.items():
                    singleton_labels[singleton_labels == old_label] = new_label
                
                # í’ˆì§ˆ í‰ê°€
                n_clusters = len(unique_labels)
                singleton_count = np.sum(singleton_labels == -1)
                
                if n_clusters > 1:
                    # SilhouetteëŠ” singleton ì œì™¸í•˜ê³  ê³„ì‚°
                    non_singleton_mask = singleton_labels != -1
                    if np.sum(non_singleton_mask) > 1:
                        silhouette = silhouette_score(
                            embeddings[non_singleton_mask], 
                            singleton_labels[non_singleton_mask]
                        )
                    else:
                        silhouette = 0
                else:
                    silhouette = 0
                
                # Singleton ë³´ë„ˆìŠ¤ í¬í•¨ ì ìˆ˜
                singleton_ratio = singleton_count / n_samples
                score = silhouette + (singleton_ratio * 0.1)
                
                if score > best_score:
                    best_labels = singleton_labels
                    best_score = score
                    best_info = {
                        'algorithm': 'kmeans_limited',
                        'k': k,
                        'n_clusters': n_clusters,
                        'singleton_count': singleton_count,
                        'singleton_ratio': singleton_ratio,
                        'silhouette': silhouette,
                        'score': score
                    }
                
                print(f"     â€¢ K={k} â†’ {n_clusters} clusters, {singleton_count} singletons, score={score:.3f}")
                
            except Exception as e:
                continue
        
        return best_labels, best_info
    
    def dbscan_singleton_aware(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Singletonì„ ê³ ë ¤í•œ DBSCAN"""
        
        n_samples = len(embeddings)
        
        best_labels = None
        best_score = -1
        best_info = {}
        
        print(f"   ğŸ”„ Trying singleton-aware DBSCAN...")
        
        # eps ë²”ìœ„ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)
        distances = 1 - similarity_matrix
        np.fill_diagonal(distances, np.inf)
        
        # ì ì ˆí•œ eps ë²”ìœ„ ê³„ì‚°
        min_distances = np.min(distances, axis=1)
        eps_candidates = np.percentile(min_distances, [10, 20, 30, 40, 50])
        
        for eps in eps_candidates:
            for min_samples in [3, 5, 7]:
                try:
                    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
                    labels = dbscan.fit_predict(embeddings)
                    
                    # í´ëŸ¬ìŠ¤í„° ì •ë³´ ê³„ì‚°
                    unique_labels = np.unique(labels)
                    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
                    singleton_count = np.sum(labels == -1)
                    
                    if n_clusters < 2:  # ë„ˆë¬´ ì ì€ í´ëŸ¬ìŠ¤í„°
                        continue
                    
                    # í’ˆì§ˆ í‰ê°€
                    if n_clusters > 1:
                        non_noise_mask = labels != -1
                        if np.sum(non_noise_mask) > 1:
                            silhouette = silhouette_score(
                                embeddings[non_noise_mask], 
                                labels[non_noise_mask]
                            )
                        else:
                            silhouette = 0
                    else:
                        silhouette = 0
                    
                    # Singleton ë¹„ìœ¨ ê³„ì‚°
                    singleton_ratio = singleton_count / n_samples
                    score = silhouette + (singleton_ratio * 0.1)
                    
                    if score > best_score:
                        best_labels = labels
                        best_score = score
                        best_info = {
                            'algorithm': 'dbscan',
                            'eps': eps,
                            'min_samples': min_samples,
                            'n_clusters': n_clusters,
                            'singleton_count': singleton_count,
                            'singleton_ratio': singleton_ratio,
                            'silhouette': silhouette,
                            'score': score
                        }
                    
                    print(f"     â€¢ eps={eps:.3f}, min_samples={min_samples}")
                    print(f"       â†’ {n_clusters} clusters, {singleton_count} singletons, score={score:.3f}")
                    
                except Exception as e:
                    continue
        
        return best_labels, best_info
    
    def fit_predict(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
        
        print(f"\nğŸ¯ Singleton-Aware Clustering ({len(embeddings)} embeddings)")
        print(f"=" * 50)
        
        # 1. MCL ì‹œë„
        mcl_labels, mcl_info = self.try_mcl_clustering(embeddings)
        
        if mcl_labels is not None:
            print(f"âœ… MCL successful - using MCL results")
            return mcl_labels, mcl_info
        
        # 2. ëŒ€ì•ˆ ì•Œê³ ë¦¬ì¦˜ë“¤ ì‹œë„
        candidates = []
        
        # 2-1. Limited K-means
        kmeans_labels, kmeans_info = self.limited_kmeans(embeddings)
        if kmeans_labels is not None:
            candidates.append((kmeans_labels, kmeans_info))
        
        # 2-2. DBSCAN
        dbscan_labels, dbscan_info = self.dbscan_singleton_aware(embeddings)
        if dbscan_labels is not None:
            candidates.append((dbscan_labels, dbscan_info))
        
        # 3. ìµœê³  ì„±ëŠ¥ ì„ íƒ
        if candidates:
            best_labels, best_info = max(candidates, key=lambda x: x[1]['score'])
            print(f"âœ… Best algorithm: {best_info['algorithm']} (score={best_info['score']:.3f})")
            return best_labels, best_info
        else:
            # 4. ìµœí›„ì˜ ìˆ˜ë‹¨: ê°„ë‹¨í•œ K-means
            print(f"âš ï¸  Fallback to simple K-means")
            kmeans = KMeans(n_clusters=3, random_state=42)
            labels = kmeans.fit_predict(embeddings)
            
            info = {
                'algorithm': 'fallback_kmeans',
                'n_clusters': 3,
                'singleton_count': 0,
                'singleton_ratio': 0,
                'silhouette': silhouette_score(embeddings, labels) if len(np.unique(labels)) > 1 else 0,
                'score': 0
            }
            
            return labels, info

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_singleton_aware_clustering():
    """Singleton-aware clustering í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª Testing Singleton-Aware Clustering")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # ì •ìƒ í´ëŸ¬ìŠ¤í„° ë°ì´í„°
    cluster1 = np.random.normal([0, 0], 0.3, (50, 2))
    cluster2 = np.random.normal([2, 2], 0.3, (50, 2))
    cluster3 = np.random.normal([-1, 2], 0.3, (30, 2))
    
    # Singleton/outlier ë°ì´í„°
    outliers = np.random.uniform(-3, 5, (10, 2))
    
    # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
    test_data = np.vstack([cluster1, cluster2, cluster3, outliers])
    
    print(f"Test data: {len(test_data)} points")
    print(f"Expected: 3 main clusters + ~10 singletons")
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    clustering = SingletonAwareClustering()
    labels, info = clustering.fit_predict(test_data)
    
    print(f"\nResults:")
    print(f"Algorithm: {info['algorithm']}")
    print(f"Clusters: {info['n_clusters']}")
    print(f"Singletons: {info['singleton_count']}")
    print(f"Silhouette: {info['silhouette']:.3f}")
    print(f"Score: {info['score']:.3f}")
    
    return labels, info

if __name__ == "__main__":
    print("ğŸ¯ Singleton-Aware Clustering Pipeline")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_labels, test_info = test_singleton_aware_clustering()
    
    print(f"\nâœ… Singleton-aware clustering module ready!")
    print(f"Key features:")
    print(f"â€¢ MCL with optimized parameters for sentence embeddings")
    print(f"â€¢ Automatic singleton detection and preservation")
    print(f"â€¢ Fallback to limited K-means/DBSCAN")
    print(f"â€¢ Quality-based algorithm selection")