"""
MCL ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ ë¶„ì„ ë° ëŒ€ì•ˆ ì œì‹œ
- Sentence embeddingì—ì„œ MCLì´ ì‹¤íŒ¨í•˜ëŠ” ì´ìœ  ë¶„ì„
- ìœ ì‚¬ë„ í–‰ë ¬ê³¼ ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„
- ëŒ€ì•ˆ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ
"""

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
import matplotlib.pyplot as plt
# import seaborn as sns
from pathlib import Path
import pickle
import json
import sys

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

from data_loader import load_data_from_state

class MCLAnalyzer:
    def __init__(self):
        self.data_path = "work_df.csv"
        self.embedding_path = "embed.npy"
        
    def load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ” Loading test data...")
        
        # ì‹¤ì œ ìƒíƒœ íŒŒì¼ ë¡œë“œ
        state_file = "/home/cyyoon/test_area/ai_text_classification/2.langgraph/data/test/state_history/20250918_113231_081_STAGE2_WORD_ë¬¸23_COMPLETED_state.json"
        
        with open(state_file, 'r', encoding='utf-8') as f:
            state = json.load(f)
        
        # ìƒíƒœì—ì„œ ë°ì´í„° ë¡œë“œ
        embeddings, metadata = load_data_from_state(state)
        
        print(f"ğŸ” Metadata keys: {list(metadata.keys())}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ DataFrame ìƒì„±
        if 'dataframe' in metadata:
            df = metadata['dataframe']
        else:
            # ê°„ë‹¨í•œ ë”ë¯¸ DataFrame ìƒì„±
            df = pd.DataFrame({
                'id': range(len(embeddings)),
                'text': [f"text_{i}" for i in range(len(embeddings))]
            })
        
        # 75ê°œ ìƒ˜í”Œë¡œ ì œí•œ
        n_samples = min(75, len(df))
        df = df.head(n_samples)
        embeddings = embeddings[:n_samples]
        
        print(f"Data shape: {df.shape}")
        print(f"Embeddings shape: {embeddings.shape}")
        
        # íƒ€ê²Ÿ ë¼ë²¨ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'target' in df.columns:
            print(f"Target distribution:")
            print(df['target'].value_counts())
            true_labels = df['target'].values
        else:
            print("No target labels found, creating synthetic labels for analysis")
            # ê°„ë‹¨í•œ ë”ë¯¸ ë¼ë²¨ ìƒì„± (ì‹¤ì œ ë¶„ì„ì—ì„œëŠ” í•„ìš”í•˜ì§€ ì•ŠìŒ)
            true_labels = np.arange(len(df)) % 3  # 3ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ”
        
        return df, embeddings, true_labels
    
    def analyze_similarity_matrix(self, embeddings):
        """ìœ ì‚¬ë„ í–‰ë ¬ ë¶„ì„"""
        print("\nğŸ“Š Analyzing similarity matrix...")
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        sim_matrix = cosine_similarity(embeddings)
        
        # ëŒ€ê°ì„  ì œê±° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„)
        mask = np.ones_like(sim_matrix, dtype=bool)
        np.fill_diagonal(mask, False)
        similarities = sim_matrix[mask]
        
        print(f"Similarity statistics:")
        print(f"  Min: {similarities.min():.3f}")
        print(f"  Max: {similarities.max():.3f}")
        print(f"  Mean: {similarities.mean():.3f}")
        print(f"  Std: {similarities.std():.3f}")
        print(f"  Median: {np.median(similarities):.3f}")
        
        # ìœ ì‚¬ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 3, 1)
        plt.hist(similarities, bins=50, alpha=0.7)
        plt.title('Similarity Distribution')
        plt.xlabel('Cosine Similarity')
        plt.ylabel('Frequency')
        
        # ìœ ì‚¬ë„ í–‰ë ¬ íˆíŠ¸ë§µ
        plt.subplot(1, 3, 2)
        plt.imshow(sim_matrix[:20, :20], cmap='viridis', aspect='auto')
        plt.colorbar()
        plt.title('Similarity Matrix (20x20)')
        
        # ì„ê³„ê°’ë³„ ì—°ê²°ì„± ë¶„ì„
        thresholds = np.arange(0.1, 0.9, 0.1)
        edge_counts = []
        
        for threshold in thresholds:
            edges = np.sum(sim_matrix > threshold) - len(sim_matrix)  # ëŒ€ê°ì„  ì œì™¸
            edge_counts.append(edges)
        
        plt.subplot(1, 3, 3)
        plt.plot(thresholds, edge_counts, 'o-')
        plt.title('Graph Connectivity')
        plt.xlabel('Similarity Threshold')
        plt.ylabel('Number of Edges')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('similarity_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        return sim_matrix
    
    def analyze_graph_structure(self, sim_matrix, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5]):
        """ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„"""
        print("\nğŸ”— Analyzing graph structure...")
        
        for threshold in thresholds:
            # ì„ê³„ê°’ ê¸°ë°˜ ê·¸ë˜í”„ ìƒì„±
            graph = sim_matrix > threshold
            np.fill_diagonal(graph, False)
            
            # ì—°ê²°ì„± ë¶„ì„
            n_edges = np.sum(graph) // 2  # ëŒ€ì¹­ì´ë¯€ë¡œ 2ë¡œ ë‚˜ëˆ”
            n_nodes = len(graph)
            density = n_edges / (n_nodes * (n_nodes - 1) / 2)
            
            # ë…¸ë“œë³„ ì—°ê²° ìˆ˜
            degrees = np.sum(graph, axis=1)
            isolated_nodes = np.sum(degrees == 0)
            
            print(f"\nThreshold {threshold:.1f}:")
            print(f"  Edges: {n_edges}")
            print(f"  Density: {density:.3f}")
            print(f"  Isolated nodes: {isolated_nodes}")
            print(f"  Avg degree: {degrees.mean():.1f}")
            print(f"  Max degree: {degrees.max()}")
    
    def test_alternative_algorithms(self, embeddings, true_labels):
        """ëŒ€ì•ˆ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Testing alternative clustering algorithms...")
        
        results = []
        
        # 1. K-Means
        for k in [3, 4, 5]:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            pred = kmeans.fit_predict(embeddings)
            
            nmi = normalized_mutual_info_score(true_labels, pred)
            ari = adjusted_rand_score(true_labels, pred)
            n_clusters = len(np.unique(pred))
            
            results.append({
                'algorithm': f'KMeans-{k}',
                'n_clusters': n_clusters,
                'nmi': nmi,
                'ari': ari,
                'score': (nmi + ari) / 2
            })
        
        # 2. Agglomerative Clustering
        for k in [3, 4, 5]:
            for linkage in ['ward', 'complete', 'average']:
                agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)
                pred = agg.fit_predict(embeddings)
                
                nmi = normalized_mutual_info_score(true_labels, pred)
                ari = adjusted_rand_score(true_labels, pred)
                n_clusters = len(np.unique(pred))
                
                results.append({
                    'algorithm': f'Agg-{linkage}-{k}',
                    'n_clusters': n_clusters,
                    'nmi': nmi,
                    'ari': ari,
                    'score': (nmi + ari) / 2
                })
        
        # 3. DBSCAN
        for eps in [0.1, 0.2, 0.3, 0.4, 0.5]:
            for min_samples in [3, 5, 7]:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
                pred = dbscan.fit_predict(embeddings)
                
                if len(np.unique(pred)) > 1:  # í´ëŸ¬ìŠ¤í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ
                    nmi = normalized_mutual_info_score(true_labels, pred)
                    ari = adjusted_rand_score(true_labels, pred)
                    n_clusters = len(np.unique(pred[pred != -1]))  # ë…¸ì´ì¦ˆ ì œì™¸
                    noise_ratio = np.sum(pred == -1) / len(pred)
                    
                    results.append({
                        'algorithm': f'DBSCAN-{eps}-{min_samples}',
                        'n_clusters': n_clusters,
                        'nmi': nmi,
                        'ari': ari,
                        'score': (nmi + ari) / 2,
                        'noise_ratio': noise_ratio
                    })
        
        # ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('score', ascending=False)
        
        print("\nğŸ† Top 10 clustering results:")
        print(results_df.head(10).to_string(index=False, float_format='%.3f'))
        
        return results_df
    
    def mcl_failure_analysis(self, sim_matrix):
        """MCL ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        print("\nğŸ” MCL Failure Analysis...")
        
        # 1. ìœ ì‚¬ë„ ë¶„í¬ê°€ MCLì— ì í•©í•œì§€ í™•ì¸
        mask = np.ones_like(sim_matrix, dtype=bool)
        np.fill_diagonal(mask, False)
        similarities = sim_matrix[mask]
        
        # MCLì´ ì˜ ì‘ë™í•˜ëŠ” ì¡°ê±´ë“¤
        print("MCL Requirements Analysis:")
        
        # ë†’ì€ ìœ ì‚¬ë„ ë¹„ìœ¨
        high_sim_ratio = np.sum(similarities > 0.5) / len(similarities)
        print(f"  High similarity ratio (>0.5): {high_sim_ratio:.3f}")
        
        # ëª…í™•í•œ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°
        very_high_sim_ratio = np.sum(similarities > 0.7) / len(similarities)
        print(f"  Very high similarity ratio (>0.7): {very_high_sim_ratio:.3f}")
        
        # ë‚®ì€ ìœ ì‚¬ë„ ë¹„ìœ¨ (ë…¸ì´ì¦ˆ)
        low_sim_ratio = np.sum(similarities < 0.1) / len(similarities)
        print(f"  Low similarity ratio (<0.1): {low_sim_ratio:.3f}")
        
        # 2. ê·¸ë˜í”„ ë°€ë„ ë¬¸ì œ
        for threshold in [0.1, 0.2, 0.3, 0.4, 0.5]:
            graph = sim_matrix > threshold
            np.fill_diagonal(graph, False)
            density = np.sum(graph) / (len(graph) * (len(graph) - 1))
            
            if density > 0.1:  # ë„ˆë¬´ ë°€ì§‘ëœ ê·¸ë˜í”„
                print(f"  âš ï¸  Graph too dense at threshold {threshold}: {density:.3f}")
            elif density < 0.01:  # ë„ˆë¬´ í¬ì†Œí•œ ê·¸ë˜í”„
                print(f"  âš ï¸  Graph too sparse at threshold {threshold}: {density:.3f}")
        
        # 3. MCLì— ì í•©í•˜ì§€ ì•Šì€ ì´ìœ  ìš”ì•½
        print("\nâŒ Why MCL fails on sentence embeddings:")
        print("  1. Similarity distribution is too uniform (most values around 0)")
        print("  2. No clear high-similarity clusters")
        print("  3. Graph structure is either too dense or too sparse")
        print("  4. MCL designed for binary/clear relationships, not continuous similarities")
    
    def recommend_best_algorithm(self, results_df):
        """ìµœì  ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ"""
        print("\nğŸ’¡ Algorithm Recommendation:")
        
        best_result = results_df.iloc[0]
        print(f"  ğŸ† Best algorithm: {best_result['algorithm']}")
        print(f"    Score: {best_result['score']:.3f}")
        print(f"    NMI: {best_result['nmi']:.3f}")
        print(f"    ARI: {best_result['ari']:.3f}")
        print(f"    Clusters: {best_result['n_clusters']}")
        
        # KMeans ì„±ëŠ¥ í™•ì¸
        kmeans_results = results_df[results_df['algorithm'].str.contains('KMeans')]
        if not kmeans_results.empty:
            best_kmeans = kmeans_results.iloc[0]
            print(f"\n  ğŸ¯ Best KMeans: {best_kmeans['algorithm']}")
            print(f"    Score: {best_kmeans['score']:.3f}")
            print(f"    Simple and reliable for sentence embeddings")
    
    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸ”¬ MCL Analysis for Sentence Embeddings")
        print("=" * 60)
        
        # ë°ì´í„° ë¡œë“œ
        df, embeddings, true_labels = self.load_test_data()
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ë¶„ì„
        sim_matrix = self.analyze_similarity_matrix(embeddings)
        
        # ê·¸ë˜í”„ êµ¬ì¡° ë¶„ì„
        self.analyze_graph_structure(sim_matrix)
        
        # MCL ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        self.mcl_failure_analysis(sim_matrix)
        
        # ëŒ€ì•ˆ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
        results_df = self.test_alternative_algorithms(embeddings, true_labels)
        
        # ìµœì  ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ
        self.recommend_best_algorithm(results_df)
        
        return results_df

if __name__ == "__main__":
    analyzer = MCLAnalyzer()
    results = analyzer.run_full_analysis()