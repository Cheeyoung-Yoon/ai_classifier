"""
Enhanced data loader for column-wise clustering
ì»¬ëŸ¼ë³„ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ë°ì´í„° ë¡œë”
"""

import ast
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional
import numpy as np
import pandas as pd

try:
    from .config import Stage3Config
except ImportError:
    from config import Stage3Config


def load_data_by_columns_from_state(state: Dict[str, Any]) -> Tuple[Dict[str, np.ndarray], Dict[str, Any]]:
    """
    ìƒíƒœì—ì„œ ì»¬ëŸ¼ë³„ë¡œ ë¶„ë¦¬ëœ ìž„ë² ë”© ë°ì´í„° ë¡œë“œ
    
    Args:
        state: LangGraph state containing matched_questions
        
    Returns:
        Tuple of (embeddings_by_column, metadata)
        embeddings_by_column: {column_name: embeddings_array}
        metadata: ë©”íƒ€ë°ì´í„° ì •ë³´
    """
    print("ðŸ” Loading data by columns from state...")
    
    if "matched_questions" not in state:
        raise ValueError("No matched_questions found in state")
    
    matched_questions = state["matched_questions"]
    embeddings_by_column = {}
    all_dataframes = []
    
    # ê° ì§ˆë¬¸ë³„ë¡œ ì²˜ë¦¬
    for question_id, question_data in matched_questions.items():
        if "stage2_data" not in question_data:
            print(f"âš ï¸  Skipping {question_id}: No stage2_data found")
            continue
        
        stage2_data = question_data["stage2_data"]
        
        if not isinstance(stage2_data, dict) or "csv_path" not in stage2_data:
            print(f"âš ï¸  Skipping {question_id}: Invalid stage2_data format")
            continue
        
        csv_path = stage2_data["csv_path"]
        
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ… Loaded {question_id}: {len(df)} rows from {Path(csv_path).name}")
            
            # ìž„ë² ë”© ì»¬ëŸ¼ ì¶”ì¶œ
            column_embeddings = extract_embeddings_by_column(df, question_id)
            
            # ì»¬ëŸ¼ë³„ ìž„ë² ë”©ì„ ì „ì²´ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€
            for col_name, embeddings in column_embeddings.items():
                full_col_name = f"{question_id}_{col_name}"
                embeddings_by_column[full_col_name] = embeddings
                print(f"   ðŸ“Š {full_col_name}: {len(embeddings)} embeddings")
            
            all_dataframes.append(df)
            
        except Exception as e:
            print(f"âŒ Error loading {question_id} from {csv_path}: {e}")
            continue
    
    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
    metadata = {
        'format': 'column_wise',
        'total_questions': len(matched_questions),
        'loaded_questions': len(all_dataframes),
        'column_info': {},
        'question_mapping': {}
    }
    
    # ì»¬ëŸ¼ ì •ë³´ ë©”íƒ€ë°ì´í„°
    for col_name, embeddings in embeddings_by_column.items():
        question_id, col_type = col_name.split('_', 1)
        metadata['column_info'][col_name] = {
            'question_id': question_id,
            'column_type': col_type,
            'embedding_count': len(embeddings),
            'embedding_dim': embeddings.shape[1] if len(embeddings) > 0 else 0
        }
        
        if question_id not in metadata['question_mapping']:
            metadata['question_mapping'][question_id] = []
        metadata['question_mapping'][question_id].append(col_name)
    
    print(f"ðŸ“Š Loaded {len(embeddings_by_column)} columns from {len(all_dataframes)} questions")
    return embeddings_by_column, metadata


def extract_embeddings_by_column(df: pd.DataFrame, question_id: str) -> Dict[str, np.ndarray]:
    """
    DataFrameì—ì„œ ì»¬ëŸ¼ë³„ë¡œ ìž„ë² ë”© ì¶”ì¶œ
    
    Args:
        df: ìž„ë² ë”©ì´ í¬í•¨ëœ DataFrame
        question_id: ì§ˆë¬¸ ID
        
    Returns:
        ì»¬ëŸ¼ë³„ ìž„ë² ë”© ë”•ì…”ë„ˆë¦¬
    """
    embeddings_by_column = {}
    
    # ìž„ë² ë”© ì»¬ëŸ¼ ì°¾ê¸°
    embedding_columns = [col for col in df.columns if 'embed' in col.lower()]
    
    print(f"ðŸ” Found embedding columns in {question_id}: {embedding_columns}")
    
    for col in embedding_columns:
        try:
            # ìž„ë² ë”© ë°ì´í„° ì¶”ì¶œ
            embeddings = []
            
            for idx, value in df[col].items():
                if pd.isna(value):
                    continue
                    
                try:
                    # ë¬¸ìžì—´ë¡œ ì €ìž¥ëœ ìž„ë² ë”©ì„ ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(value, str):
                        embedding = ast.literal_eval(value)
                    else:
                        embedding = value
                    
                    # numpy ë°°ì—´ë¡œ ë³€í™˜
                    embedding_array = np.array(embedding, dtype=np.float32)
                    
                    # 1ì°¨ì› ë°°ì—´ì¸ì§€ í™•ì¸
                    if embedding_array.ndim == 1 and len(embedding_array) > 0:
                        embeddings.append(embedding_array)
                        
                except Exception as e:
                    continue  # ë³€í™˜ ì‹¤íŒ¨í•œ ìž„ë² ë”©ì€ ìŠ¤í‚µ
            
            if embeddings:
                # ëª¨ë“  ìž„ë² ë”©ì„ 2D ë°°ì—´ë¡œ ë³€í™˜
                embeddings_array = np.vstack(embeddings)
                
                # ì»¬ëŸ¼ëª… ì •ë¦¬ (embed_ ì œê±°)
                clean_col_name = col.replace('embed_', '').replace('_embed', '')
                embeddings_by_column[clean_col_name] = embeddings_array
                
                print(f"   âœ… {clean_col_name}: {len(embeddings_array)} embeddings, dim={embeddings_array.shape[1]}")
            else:
                print(f"   âš ï¸  No valid embeddings found in column: {col}")
                
        except Exception as e:
            print(f"   âŒ Error processing column {col}: {e}")
            continue
    
    return embeddings_by_column


def group_columns_by_type(embeddings_by_column: Dict[str, np.ndarray]) -> Dict[str, Dict[str, np.ndarray]]:
    """
    ì»¬ëŸ¼ë“¤ì„ íƒ€ìž…ë³„ë¡œ ê·¸ë£¹í™”
    
    Args:
        embeddings_by_column: ì»¬ëŸ¼ë³„ ìž„ë² ë”© ë”•ì…”ë„ˆë¦¬
        
    Returns:
        íƒ€ìž…ë³„ë¡œ ê·¸ë£¹í™”ëœ ìž„ë² ë”© ë”•ì…”ë„ˆë¦¬
    """
    grouped = {}
    
    for col_name, embeddings in embeddings_by_column.items():
        # ì§ˆë¬¸IDì™€ ì»¬ëŸ¼íƒ€ìž… ë¶„ë¦¬
        parts = col_name.split('_', 1)
        if len(parts) == 2:
            question_id, col_type = parts
        else:
            question_id = parts[0]
            col_type = 'unknown'
        
        # ì»¬ëŸ¼ íƒ€ìž…ë³„ë¡œ ê·¸ë£¹í™”
        if col_type not in grouped:
            grouped[col_type] = {}
        
        grouped[col_type][col_name] = embeddings
    
    return grouped


def select_reference_column(embeddings_by_column: Dict[str, np.ndarray], 
                           strategy: str = 'largest') -> str:
    """
    ê¸°ì¤€ ì»¬ëŸ¼ ì„ íƒ
    
    Args:
        embeddings_by_column: ì»¬ëŸ¼ë³„ ìž„ë² ë”© ë”•ì…”ë„ˆë¦¬
        strategy: ì„ íƒ ì „ëžµ ('largest', 'first', 'most_diverse')
        
    Returns:
        ì„ íƒëœ ê¸°ì¤€ ì»¬ëŸ¼ëª…
    """
    if not embeddings_by_column:
        raise ValueError("No columns available")
    
    if strategy == 'largest':
        # ê°€ìž¥ ë§Žì€ ìƒ˜í”Œì„ ê°€ì§„ ì»¬ëŸ¼ ì„ íƒ
        return max(embeddings_by_column.keys(), 
                  key=lambda k: len(embeddings_by_column[k]))
    
    elif strategy == 'first':
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì„ íƒ
        return list(embeddings_by_column.keys())[0]
    
    elif strategy == 'most_diverse':
        # ê°€ìž¥ ë‹¤ì–‘ì„±ì´ ë†’ì€ ì»¬ëŸ¼ ì„ íƒ (ë¶„ì‚°ì´ í° ì»¬ëŸ¼)
        max_variance = -1
        best_column = None
        
        for col_name, embeddings in embeddings_by_column.items():
            if len(embeddings) > 10:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
                variance = np.var(embeddings, axis=0).mean()
                if variance > max_variance:
                    max_variance = variance
                    best_column = col_name
        
        return best_column or list(embeddings_by_column.keys())[0]
    
    else:
        return list(embeddings_by_column.keys())[0]


if __name__ == "__main__":
    # Test the column-wise data loader
    import json
    
    print("ðŸ§ª Testing Column-wise Data Loader")
    print("=" * 50)
    
    # Load test state
    state_file = "/home/cyyoon/test_area/ai_text_classification/2.langgraph/data/test/state_history/20250918_113231_081_STAGE2_WORD_ë¬¸23_COMPLETED_state.json"
    
    with open(state_file, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    # Test column-wise loading
    embeddings_by_column, metadata = load_data_by_columns_from_state(state)
    
    print(f"\nðŸ“Š Loaded Columns:")
    for col_name, embeddings in embeddings_by_column.items():
        print(f"   {col_name}: {embeddings.shape}")
    
    # Test column grouping
    grouped = group_columns_by_type(embeddings_by_column)
    print(f"\nðŸ“‚ Grouped by type:")
    for col_type, columns in grouped.items():
        print(f"   {col_type}: {len(columns)} columns")
        for col_name in columns.keys():
            print(f"     - {col_name}")
    
    # Test reference column selection
    ref_col = select_reference_column(embeddings_by_column, 'largest')
    print(f"\nðŸŽ¯ Selected reference column: {ref_col}")
    
    print("\nâœ… Column-wise data loader test completed!")