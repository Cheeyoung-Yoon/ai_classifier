"""
NMI & ARI Based Clustering Evaluation System
Silhouette score ëŒ€ì‹  NMI(Normalized Mutual Information)ì™€ ARI(Adjusted Rand Index) ê¸°ë°˜ í‰ê°€
"""

import numpy as np
import pandas as pd
from typing import Dict, Any, List, Tuple, Optional
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

class NMIARIEvaluator:
    """NMIì™€ ARI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ í‰ê°€ì"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {
            'nmi_weight': 0.6,  # NMI ê°€ì¤‘ì¹˜
            'ari_weight': 0.4,  # ARI ê°€ì¤‘ì¹˜
            'min_cluster_size_ratio': 0.03,  # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ì „ì²´ì˜ 3%)
            'max_clusters': 7,   # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜
            'min_nmi_threshold': 0.1,  # ìµœì†Œ NMI ì„ê³„ê°’
            'min_ari_threshold': 0.05  # ìµœì†Œ ARI ì„ê³„ê°’
        }
        
    def create_synthetic_ground_truth(self, embeddings: np.ndarray, method: str = 'kmeans') -> np.ndarray:
        """
        ì‹¤ì œ ground truthê°€ ì—†ì„ ë•Œ synthetic ground truth ìƒì„±
        ì‹¤ì œ ë°ì´í„°ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ì¶”ì •
        """
        n_samples = len(embeddings)
        
        if method == 'kmeans':
            # K-meansë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¶”ì • (3-5ê°œ)
            best_k = 3
            best_inertia = float('inf')
            
            for k in range(3, min(6, n_samples//10)):  # ìµœì†Œ 10ê°œì”©ì€ ìˆì–´ì•¼ í•¨
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(embeddings)
                
                if kmeans.inertia_ < best_inertia:
                    best_inertia = kmeans.inertia_
                    best_k = k
            
            # ìµœì  Kë¡œ ground truth ìƒì„±
            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
            ground_truth = kmeans.fit_predict(embeddings)
            
        elif method == 'hierarchical':
            # Hierarchical clusteringìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì¡° ì°¾ê¸°
            n_clusters = min(5, max(3, n_samples//20))
            hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
            ground_truth = hierarchical.fit_predict(embeddings)
            
        elif method == 'consensus':
            # ì—¬ëŸ¬ ë°©ë²•ì˜ consensus
            k1 = KMeans(n_clusters=3, random_state=42).fit_predict(embeddings)
            k2 = KMeans(n_clusters=4, random_state=43).fit_predict(embeddings)
            k3 = KMeans(n_clusters=5, random_state=44).fit_predict(embeddings)
            
            # ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ ì„ íƒ (ê°„ë‹¨íˆ ì²« ë²ˆì§¸ ì‚¬ìš©)
            ground_truth = k1
            
        return ground_truth
    
    def calculate_nmi_ari(self, true_labels: np.ndarray, pred_labels: np.ndarray) -> Dict[str, float]:
        """NMIì™€ ARI ê³„ì‚°"""
        
        # ìœ íš¨í•œ ë¼ë²¨ë§Œ ì‚¬ìš© (-1 ì œì™¸)
        valid_mask = (pred_labels != -1) & (true_labels != -1)
        
        if np.sum(valid_mask) < 2:
            return {
                'nmi': 0.0,
                'ari': 0.0,
                'adj_ari': 0.0,
                'combined_score': 0.0,
                'valid_samples': 0
            }
        
        valid_true = true_labels[valid_mask]
        valid_pred = pred_labels[valid_mask]
        
        # NMI ê³„ì‚°
        nmi = normalized_mutual_info_score(valid_true, valid_pred)
        
        # ARI ê³„ì‚°
        ari = adjusted_rand_score(valid_true, valid_pred)
        
        # Adjusted ARI (ì „ì²´ í•­ëª© ìˆ˜ ê³ ë ¤)
        total_samples = len(true_labels)
        valid_samples = len(valid_true)
        coverage_penalty = valid_samples / total_samples  # ì»¤ë²„ë¦¬ì§€ íŒ¨ë„í‹°
        
        adj_ari = ari * coverage_penalty
        
        # Combined score (NMI + ARI ê°€ì¤‘ í‰ê· )
        combined_score = (
            self.config['nmi_weight'] * nmi + 
            self.config['ari_weight'] * adj_ari
        )
        
        return {
            'nmi': nmi,
            'ari': ari,
            'adj_ari': adj_ari,
            'combined_score': combined_score,
            'valid_samples': valid_samples,
            'coverage_ratio': coverage_penalty
        }
    
    def evaluate_clustering_quality(self, embeddings: np.ndarray, pred_labels: np.ndarray, 
                                   ground_truth: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
        
        # Ground truthê°€ ì—†ìœ¼ë©´ ìƒì„±
        if ground_truth is None:
            ground_truth = self.create_synthetic_ground_truth(embeddings, method='consensus')
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self.calculate_nmi_ari(ground_truth, pred_labels)
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„
        unique_labels = np.unique(pred_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = np.sum(pred_labels == -1)
        n_samples = len(pred_labels)
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
        cluster_sizes = []
        for label in unique_labels:
            if label != -1:
                cluster_sizes.append(np.sum(pred_labels == label))
        
        # í’ˆì§ˆ íŒì •
        quality_assessment = self._assess_quality(metrics, n_clusters, n_samples, cluster_sizes)
        
        return {
            **metrics,
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'n_samples': n_samples,
            'cluster_sizes': cluster_sizes,
            'avg_cluster_size': np.mean(cluster_sizes) if cluster_sizes else 0,
            'min_cluster_size': np.min(cluster_sizes) if cluster_sizes else 0,
            'max_cluster_size': np.max(cluster_sizes) if cluster_sizes else 0,
            'quality_assessment': quality_assessment
        }
    
    def _assess_quality(self, metrics: Dict, n_clusters: int, n_samples: int, 
                       cluster_sizes: List[int]) -> str:
        """í’ˆì§ˆ ì¢…í•© íŒì •"""
        
        nmi = metrics['nmi']
        adj_ari = metrics['adj_ari']
        combined = metrics['combined_score']
        
        # ì„ê³„ê°’ ì²´í¬
        nmi_good = nmi >= self.config['min_nmi_threshold']
        ari_good = adj_ari >= self.config['min_ari_threshold']
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì²´í¬
        clusters_reasonable = 2 <= n_clusters <= self.config['max_clusters']
        
        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ì²´í¬
        min_size = n_samples * self.config['min_cluster_size_ratio']
        sizes_good = all(size >= min_size for size in cluster_sizes) if cluster_sizes else False
        
        if nmi_good and ari_good and clusters_reasonable and sizes_good:
            if combined >= 0.5:
                return "EXCELLENT"
            elif combined >= 0.3:
                return "GOOD"
            else:
                return "ACCEPTABLE"
        elif nmi_good and ari_good:
            return "FAIR"
        else:
            return "POOR"

def test_nmi_ari_evaluator():
    """NMI/ARI í‰ê°€ì í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª Testing NMI/ARI Evaluator")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # ëª…í™•í•œ 3ê°œ í´ëŸ¬ìŠ¤í„°
    cluster1 = np.random.normal([0, 0], 0.5, (100, 2))
    cluster2 = np.random.normal([3, 3], 0.5, (100, 2))
    cluster3 = np.random.normal([-2, 3], 0.5, (80, 2))
    
    # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë“¤
    noise = np.random.uniform(-5, 5, (20, 2))
    
    test_data = np.vstack([cluster1, cluster2, cluster3, noise])
    
    # Ground truth (ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨)
    true_labels = np.array([0]*100 + [1]*100 + [2]*80 + [3]*20)  # ë§ˆì§€ë§‰ì€ ë…¸ì´ì¦ˆ
    
    print(f"Test data: {len(test_data)} points, 3 main clusters + noise")
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = NMIARIEvaluator()
    
    # í…ŒìŠ¤íŠ¸ 1: ì™„ë²½í•œ í´ëŸ¬ìŠ¤í„°ë§
    print(f"\nğŸ“Š Test 1: Perfect Clustering")
    perfect_labels = true_labels.copy()
    perfect_results = evaluator.evaluate_clustering_quality(test_data, perfect_labels, true_labels)
    
    print(f"   NMI: {perfect_results['nmi']:.3f}")
    print(f"   ARI: {perfect_results['ari']:.3f}")
    print(f"   Adj ARI: {perfect_results['adj_ari']:.3f}")
    print(f"   Combined Score: {perfect_results['combined_score']:.3f}")
    print(f"   Quality: {perfect_results['quality_assessment']}")
    
    # í…ŒìŠ¤íŠ¸ 2: K-means í´ëŸ¬ìŠ¤í„°ë§
    print(f"\nğŸ“Š Test 2: K-means Clustering")
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans_labels = kmeans.fit_predict(test_data)
    kmeans_results = evaluator.evaluate_clustering_quality(test_data, kmeans_labels, true_labels)
    
    print(f"   NMI: {kmeans_results['nmi']:.3f}")
    print(f"   ARI: {kmeans_results['ari']:.3f}")
    print(f"   Adj ARI: {kmeans_results['adj_ari']:.3f}")
    print(f"   Combined Score: {kmeans_results['combined_score']:.3f}")
    print(f"   Quality: {kmeans_results['quality_assessment']}")
    print(f"   Clusters: {kmeans_results['n_clusters']}")
    
    # í…ŒìŠ¤íŠ¸ 3: DBSCAN í´ëŸ¬ìŠ¤í„°ë§
    print(f"\nğŸ“Š Test 3: DBSCAN Clustering")
    dbscan = DBSCAN(eps=0.8, min_samples=5)
    dbscan_labels = dbscan.fit_predict(test_data)
    dbscan_results = evaluator.evaluate_clustering_quality(test_data, dbscan_labels, true_labels)
    
    print(f"   NMI: {dbscan_results['nmi']:.3f}")
    print(f"   ARI: {dbscan_results['ari']:.3f}")
    print(f"   Adj ARI: {dbscan_results['adj_ari']:.3f}")
    print(f"   Combined Score: {dbscan_results['combined_score']:.3f}")
    print(f"   Quality: {dbscan_results['quality_assessment']}")
    print(f"   Clusters: {dbscan_results['n_clusters']}, Noise: {dbscan_results['n_noise']}")
    
    # í…ŒìŠ¤íŠ¸ 4: ë‚˜ìœ í´ëŸ¬ìŠ¤í„°ë§ (ëª¨ë“  ì ì„ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ)
    print(f"\nğŸ“Š Test 4: Poor Clustering (All Same)")
    poor_labels = np.zeros(len(test_data))
    poor_results = evaluator.evaluate_clustering_quality(test_data, poor_labels, true_labels)
    
    print(f"   NMI: {poor_results['nmi']:.3f}")
    print(f"   ARI: {poor_results['ari']:.3f}")
    print(f"   Adj ARI: {poor_results['adj_ari']:.3f}")
    print(f"   Combined Score: {poor_results['combined_score']:.3f}")
    print(f"   Quality: {poor_results['quality_assessment']}")
    
    return {
        'perfect': perfect_results,
        'kmeans': kmeans_results,
        'dbscan': dbscan_results,
        'poor': poor_results
    }

def demonstrate_adj_ari():
    """Adjusted ARIì˜ ì „ì²´ í•­ëª© ìˆ˜ ê²€ì¦ ê¸°ëŠ¥ ì‹œì—°"""
    
    print(f"\nğŸ” Adjusted ARI - ì „ì²´ í•­ëª© ìˆ˜ ê²€ì¦")
    print("=" * 50)
    
    evaluator = NMIARIEvaluator()
    
    # ì‹œë‚˜ë¦¬ì˜¤ 1: ëª¨ë“  í•­ëª©ì´ í´ëŸ¬ìŠ¤í„°ë§ë¨
    true_labels_1 = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
    pred_labels_1 = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2])  # ëª¨ë“  í•­ëª© í¬í•¨
    
    result_1 = evaluator.calculate_nmi_ari(true_labels_1, pred_labels_1)
    print(f"ì‹œë‚˜ë¦¬ì˜¤ 1 - ì „ì²´ í•­ëª© í´ëŸ¬ìŠ¤í„°ë§:")
    print(f"   ARI: {result_1['ari']:.3f}")
    print(f"   Adj ARI: {result_1['adj_ari']:.3f}")
    print(f"   Coverage: {result_1['coverage_ratio']:.3f}")
    print(f"   Valid samples: {result_1['valid_samples']}/9")
    
    # ì‹œë‚˜ë¦¬ì˜¤ 2: ì¼ë¶€ í•­ëª©ì´ ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜ë¨
    true_labels_2 = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
    pred_labels_2 = np.array([0, 0, 1, 1, 1, 2, 2, -1, -1])  # 2ê°œ ë…¸ì´ì¦ˆ
    
    result_2 = evaluator.calculate_nmi_ari(true_labels_2, pred_labels_2)
    print(f"\nì‹œë‚˜ë¦¬ì˜¤ 2 - ì¼ë¶€ ë…¸ì´ì¦ˆ í¬í•¨:")
    print(f"   ARI: {result_2['ari']:.3f}")
    print(f"   Adj ARI: {result_2['adj_ari']:.3f}")
    print(f"   Coverage: {result_2['coverage_ratio']:.3f}")
    print(f"   Valid samples: {result_2['valid_samples']}/9")
    
    # ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ë¶€ë¶„ ë…¸ì´ì¦ˆ
    true_labels_3 = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
    pred_labels_3 = np.array([0, 0, -1, -1, -1, -1, -1, -1, -1])  # ëŒ€ë¶€ë¶„ ë…¸ì´ì¦ˆ
    
    result_3 = evaluator.calculate_nmi_ari(true_labels_3, pred_labels_3)
    print(f"\nì‹œë‚˜ë¦¬ì˜¤ 3 - ëŒ€ë¶€ë¶„ ë…¸ì´ì¦ˆ:")
    print(f"   ARI: {result_3['ari']:.3f}")
    print(f"   Adj ARI: {result_3['adj_ari']:.3f}")
    print(f"   Coverage: {result_3['coverage_ratio']:.3f}")
    print(f"   Valid samples: {result_3['valid_samples']}/9")
    
    print(f"\nğŸ’¡ Adjusted ARI íš¨ê³¼:")
    print(f"   - ì „ì²´ í•­ëª© ëŒ€ë¹„ ìœ íš¨ í´ëŸ¬ìŠ¤í„°ë§ ë¹„ìœ¨ ë°˜ì˜")
    print(f"   - ë…¸ì´ì¦ˆê°€ ë§ì„ìˆ˜ë¡ ì ìˆ˜ í•˜ë½")
    print(f"   - ì‹¤ì œ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í’ˆì§ˆ ì¸¡ì •")

if __name__ == "__main__":
    print("ğŸ¯ NMI & ARI Based Clustering Evaluation")
    print("=" * 50)
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    test_results = test_nmi_ari_evaluator()
    
    # Adjusted ARI ì‹œì—°
    demonstrate_adj_ari()
    
    print(f"\nâœ… NMI/ARI Evaluation System Ready!")
    print(f"Key Features:")
    print(f"â€¢ NMI (Normalized Mutual Information) - ì •ë³´ ì´ë¡  ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ")
    print(f"â€¢ ARI (Adjusted Rand Index) - ëœë¤ ë³´ì •ëœ í´ëŸ¬ìŠ¤í„° ì¼ì¹˜ë„")
    print(f"â€¢ Adj ARI - ì „ì²´ í•­ëª© ìˆ˜ ê³ ë ¤í•œ ë³´ì •ëœ ARI")
    print(f"â€¢ Combined Score - NMI + ARI ê°€ì¤‘ ê²°í•© ì ìˆ˜")
    print(f"â€¢ Quality Assessment - ì¢…í•© í’ˆì§ˆ ë“±ê¸‰ (EXCELLENT ~ POOR)")