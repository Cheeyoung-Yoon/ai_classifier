"""
Question-wise Classification Pipeline
ê° ë¬¸í•­(ì§ˆë¬¸)ë³„ë¡œ ê°œë³„ì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸
"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from typing import Dict, Any, Tuple, List, Optional, Union
import json
from pathlib import Path
from collections import defaultdict

class QuestionWiseClassification:
    """
    ë¬¸í•­ë³„ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸
    ê° ë¬¸í•­ì˜ ì»¬ëŸ¼ë“¤ë§Œ ì‚¬ìš©í•´ì„œ í•´ë‹¹ ë¬¸í•­ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize question-wise classification pipeline
        
        Args:
            config: Configuration dictionary
        """
        self.config = config or self._get_default_config()
        
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration for question-wise clustering"""
        return {
            'algorithm': 'adaptive',  # adaptive, kmeans, dbscan, hierarchical
            'kmeans_k_range': [3, 4, 5, 6, 7],
            'dbscan_eps_range': [0.1, 0.2, 0.3, 0.4],
            'dbscan_min_samples_range': [3, 5, 7],
            'hierarchical_k_range': [3, 4, 5, 6, 7],
            'hierarchical_linkage': ['ward', 'complete', 'average'],
            'max_clusters': 10,
            'min_samples_per_question': 20,  # ë¬¸í•­ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            'selection_criteria': 'silhouette',
            'clustering_strategy': 'individual_columns',  # individual_columns, combined_embeddings
            'reference_column_strategy': 'largest'  # largest, first, most_diverse
        }
    
    def cluster_by_questions(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê° ë¬¸í•­ë³„ë¡œ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            state: LangGraph state containing matched_questions
            
        Returns:
            ë¬¸í•­ë³„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        print(f"ğŸ”§ Question-wise clustering...")
        
        if "matched_questions" not in state:
            raise ValueError("No matched_questions found in state")
        
        matched_questions = state["matched_questions"]
        question_results = {}
        
        # ê° ë¬¸í•­ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        for question_id, question_data in matched_questions.items():
            print(f"\nğŸ“Š Processing question: {question_id}")
            
            try:
                result = self._cluster_single_question(question_id, question_data)
                if result is not None:
                    question_results[question_id] = result
                    print(f"   âœ… {question_id}: Clustering completed")
                else:
                    print(f"   âš ï¸  {question_id}: Clustering skipped")
            except Exception as e:
                print(f"   âŒ {question_id}: Error - {e}")
                continue
        
        return {
            'question_results': question_results,
            'total_questions': len(matched_questions),
            'processed_questions': len(question_results),
            'clustering_strategy': self.config['clustering_strategy']
        }
    
    def _cluster_single_question(self, question_id: str, question_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ë‹¨ì¼ ë¬¸í•­ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            question_id: ë¬¸í•­ ID
            question_data: ë¬¸í•­ ë°ì´í„°
            
        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë˜ëŠ” None
        """
        # Stage2 ë°ì´í„° í™•ì¸
        if "stage2_data" not in question_data:
            print(f"   âš ï¸  No stage2_data found for {question_id}")
            return None
        
        stage2_data = question_data["stage2_data"]
        if not isinstance(stage2_data, dict) or "csv_path" not in stage2_data:
            print(f"   âš ï¸  Invalid stage2_data format for {question_id}")
            return None
        
        csv_path = stage2_data["csv_path"]
        
        try:
            # CSV ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(csv_path)
            print(f"   ğŸ“ Loaded: {len(df)} rows from {Path(csv_path).name}")
            
            # ì„ë² ë”© ì»¬ëŸ¼ë³„ë¡œ ì¶”ì¶œ
            embeddings_by_column = self._extract_question_embeddings(df, question_id)
            
            if not embeddings_by_column:
                print(f"   âš ï¸  No valid embeddings found for {question_id}")
                return None
            
            # í´ëŸ¬ìŠ¤í„°ë§ ì „ëµì— ë”°ë¼ ì²˜ë¦¬
            if self.config['clustering_strategy'] == 'individual_columns':
                return self._cluster_individual_columns(question_id, embeddings_by_column, df)
            elif self.config['clustering_strategy'] == 'combined_embeddings':
                return self._cluster_combined_embeddings(question_id, embeddings_by_column, df)
            else:
                print(f"   âŒ Unknown clustering strategy: {self.config['clustering_strategy']}")
                return None
                
        except Exception as e:
            print(f"   âŒ Error processing {question_id}: {e}")
            return None
    
    def _extract_question_embeddings(self, df: pd.DataFrame, question_id: str) -> Dict[str, np.ndarray]:
        """
        ë¬¸í•­ì˜ DataFrameì—ì„œ ì„ë² ë”© ì¶”ì¶œ
        
        Args:
            df: ë¬¸í•­ DataFrame
            question_id: ë¬¸í•­ ID
            
        Returns:
            ì»¬ëŸ¼ë³„ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        """
        embeddings_by_column = {}
        
        # ì„ë² ë”© ì»¬ëŸ¼ ì°¾ê¸°
        embedding_columns = [col for col in df.columns if 'embed' in col.lower()]
        print(f"   ğŸ” Found {len(embedding_columns)} embedding columns")
        
        for col in embedding_columns:
            try:
                embeddings = []
                
                for idx, value in df[col].items():
                    if pd.isna(value):
                        continue
                        
                    try:
                        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ì„ë² ë”©ì„ ë°°ì—´ë¡œ ë³€í™˜
                        if isinstance(value, str):
                            import ast
                            embedding = ast.literal_eval(value)
                        else:
                            embedding = value
                        
                        # numpy ë°°ì—´ë¡œ ë³€í™˜
                        embedding_array = np.array(embedding, dtype=np.float32)
                        
                        # 1ì°¨ì› ë°°ì—´ì¸ì§€ í™•ì¸
                        if embedding_array.ndim == 1 and len(embedding_array) > 0:
                            embeddings.append(embedding_array)
                            
                    except Exception as e:
                        continue  # ë³€í™˜ ì‹¤íŒ¨í•œ ì„ë² ë”©ì€ ìŠ¤í‚µ
                
                if embeddings:
                    # ëª¨ë“  ì„ë² ë”©ì„ 2D ë°°ì—´ë¡œ ë³€í™˜
                    embeddings_array = np.vstack(embeddings)
                    
                    # ì»¬ëŸ¼ëª… ì •ë¦¬
                    clean_col_name = col.replace('embed_', '').replace('_embed', '')
                    embeddings_by_column[clean_col_name] = embeddings_array
                    
                    print(f"     ğŸ“Š {clean_col_name}: {len(embeddings_array)} embeddings")
                    
            except Exception as e:
                print(f"     âŒ Error processing column {col}: {e}")
                continue
        
        return embeddings_by_column
    
    def _cluster_individual_columns(self, question_id: str, 
                                   embeddings_by_column: Dict[str, np.ndarray],
                                   df: pd.DataFrame) -> Dict[str, Any]:
        """
        ê° ì»¬ëŸ¼ë³„ë¡œ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            question_id: ë¬¸í•­ ID
            embeddings_by_column: ì»¬ëŸ¼ë³„ ì„ë² ë”©
            df: ì›ë³¸ DataFrame
            
        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        print(f"   ğŸ¯ Individual column clustering for {question_id}")
        
        column_results = {}
        
        # ê° ì»¬ëŸ¼ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        for col_name, embeddings in embeddings_by_column.items():
            if len(embeddings) < self.config['min_samples_per_question']:
                print(f"     âš ï¸  Skipping {col_name}: insufficient samples ({len(embeddings)})")
                continue
            
            try:
                # ê°œë³„ ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§
                result = self._cluster_single_column(embeddings, f"{question_id}_{col_name}")
                if result is not None:
                    column_results[col_name] = result
                    print(f"     âœ… {col_name}: {result['n_clusters']} clusters")
                    
            except Exception as e:
                print(f"     âŒ Error clustering {col_name}: {e}")
                continue
        
        if not column_results:
            return None
        
        # ê¸°ì¤€ ì»¬ëŸ¼ ì„ íƒ
        reference_column = self._select_reference_column(embeddings_by_column)
        print(f"   ğŸ“Œ Reference column: {reference_column}")
        
        return {
            'question_id': question_id,
            'clustering_type': 'individual_columns',
            'column_results': column_results,
            'reference_column': reference_column,
            'total_columns': len(embeddings_by_column),
            'clustered_columns': len(column_results),
            'original_dataframe_rows': len(df)
        }
    
    def _cluster_combined_embeddings(self, question_id: str,
                                   embeddings_by_column: Dict[str, np.ndarray],
                                   df: pd.DataFrame) -> Dict[str, Any]:
        """
        ëª¨ë“  ì»¬ëŸ¼ì˜ ì„ë² ë”©ì„ í•©ì³ì„œ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            question_id: ë¬¸í•­ ID
            embeddings_by_column: ì»¬ëŸ¼ë³„ ì„ë² ë”©
            df: ì›ë³¸ DataFrame
            
        Returns:
            í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        print(f"   ğŸ¯ Combined embeddings clustering for {question_id}")
        
        # ëª¨ë“  ì„ë² ë”©ì„ í•©ì¹˜ê¸°
        all_embeddings = []
        embedding_mapping = []  # (row_idx, col_name) ë§¤í•‘
        
        for col_name, embeddings in embeddings_by_column.items():
            for i, embedding in enumerate(embeddings):
                all_embeddings.append(embedding)
                embedding_mapping.append((i, col_name))
        
        if len(all_embeddings) < self.config['min_samples_per_question']:
            print(f"     âš ï¸  Insufficient total samples: {len(all_embeddings)}")
            return None
        
        # í•©ì³ì§„ ì„ë² ë”©ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        combined_embeddings = np.vstack(all_embeddings)
        result = self._cluster_single_column(combined_embeddings, question_id)
        
        if result is None:
            return None
        
        # ê²°ê³¼ë¥¼ ì»¬ëŸ¼ë³„ë¡œ ë¶„ë¦¬
        column_labels = {}
        for i, (row_idx, col_name) in enumerate(embedding_mapping):
            if col_name not in column_labels:
                column_labels[col_name] = []
            column_labels[col_name].append(result['labels'][i])
        
        return {
            'question_id': question_id,
            'clustering_type': 'combined_embeddings',
            'combined_result': result,
            'column_labels': column_labels,
            'embedding_mapping': embedding_mapping,
            'total_embeddings': len(all_embeddings),
            'original_dataframe_rows': len(df)
        }
    
    def _cluster_single_column(self, embeddings: np.ndarray, name: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì»¬ëŸ¼/ë°ì´í„°ì…‹ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            from optimized_classification import OptimizedClassification
            
            # ì»¬ëŸ¼ìš© ì„¤ì •
            column_config = self.config.copy()
            column_config['max_clusters'] = min(self.config['max_clusters'], len(embeddings) // 3)
            
            classifier = OptimizedClassification(column_config)
            result = classifier.cluster_embeddings(embeddings)
            
            if result is not None:
                result['name'] = name
                result['sample_count'] = len(embeddings)
                return result
                
        except Exception as e:
            print(f"     âŒ Error clustering {name}: {e}")
            
        return None
    
    def _select_reference_column(self, embeddings_by_column: Dict[str, np.ndarray]) -> str:
        """ê¸°ì¤€ ì»¬ëŸ¼ ì„ íƒ"""
        if not embeddings_by_column:
            raise ValueError("No columns available")
        
        strategy = self.config['reference_column_strategy']
        
        if strategy == 'largest':
            return max(embeddings_by_column.keys(), 
                      key=lambda k: len(embeddings_by_column[k]))
        elif strategy == 'first':
            return list(embeddings_by_column.keys())[0]
        elif strategy == 'most_diverse':
            max_variance = -1
            best_column = None
            
            for col_name, embeddings in embeddings_by_column.items():
                if len(embeddings) > 10:
                    variance = np.var(embeddings, axis=0).mean()
                    if variance > max_variance:
                        max_variance = variance
                        best_column = col_name
            
            return best_column or list(embeddings_by_column.keys())[0]
        else:
            return list(embeddings_by_column.keys())[0]
    
    def create_results_summary(self, results: Dict[str, Any]) -> pd.DataFrame:
        """
        ê²°ê³¼ ìš”ì•½ DataFrame ìƒì„±
        
        Args:
            results: cluster_by_questions ê²°ê³¼
            
        Returns:
            ìš”ì•½ DataFrame
        """
        summary_data = []
        
        for question_id, question_result in results['question_results'].items():
            base_info = {
                'question_id': question_id,
                'clustering_type': question_result['clustering_type'],
                'original_rows': question_result['original_dataframe_rows']
            }
            
            if question_result['clustering_type'] == 'individual_columns':
                # ê°œë³„ ì»¬ëŸ¼ ê²°ê³¼
                for col_name, col_result in question_result['column_results'].items():
                    row_data = base_info.copy()
                    row_data.update({
                        'column_name': col_name,
                        'n_clusters': col_result['n_clusters'],
                        'algorithm': col_result.get('selected_algorithm', 'unknown'),
                        'silhouette_score': col_result.get('silhouette', -1),
                        'sample_count': col_result['sample_count']
                    })
                    summary_data.append(row_data)
            
            elif question_result['clustering_type'] == 'combined_embeddings':
                # í•©ì³ì§„ ê²°ê³¼
                combined_result = question_result['combined_result']
                row_data = base_info.copy()
                row_data.update({
                    'column_name': 'combined',
                    'n_clusters': combined_result['n_clusters'],
                    'algorithm': combined_result.get('selected_algorithm', 'unknown'),
                    'silhouette_score': combined_result.get('silhouette', -1),
                    'sample_count': question_result['total_embeddings']
                })
                summary_data.append(row_data)
        
        return pd.DataFrame(summary_data)


def create_question_wise_classification_pipeline(config: Optional[Dict] = None) -> QuestionWiseClassification:
    """Create question-wise classification pipeline"""
    return QuestionWiseClassification(config)


if __name__ == "__main__":
    # Test the question-wise pipeline
    import json
    
    print("ğŸ§ª Testing Question-wise Classification Pipeline")
    print("=" * 60)
    
    # Load test data
    state_file = "/home/cyyoon/test_area/ai_text_classification/2.langgraph/data/test/state_history/20250918_113231_081_STAGE2_WORD_ë¬¸23_COMPLETED_state.json"
    
    with open(state_file, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    # Test question-wise clustering
    classifier = create_question_wise_classification_pipeline()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì„¤ì • ì¡°ì •
    classifier.config.update({
        'min_samples_per_question': 10,
        'kmeans_k_range': [3, 4, 5],
        'clustering_strategy': 'individual_columns'
    })
    
    results = classifier.cluster_by_questions(state)
    
    print(f"\nğŸ¯ Question-wise Clustering Results:")
    print(f"   Processed questions: {results['processed_questions']}/{results['total_questions']}")
    print(f"   Strategy: {results['clustering_strategy']}")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    summary_df = classifier.create_results_summary(results)
    print(f"\nğŸ“Š Results Summary:")
    if not summary_df.empty:
        print(summary_df.to_string(index=False))
    else:
        print("   No clustering results to display")
    
    # ê° ë¬¸í•­ë³„ ìƒì„¸ ê²°ê³¼
    for question_id, question_result in results['question_results'].items():
        print(f"\nğŸ“‹ {question_id} Details:")
        if question_result['clustering_type'] == 'individual_columns':
            for col_name, col_result in question_result['column_results'].items():
                print(f"   {col_name}: {col_result['n_clusters']} clusters, "
                      f"algorithm={col_result.get('selected_algorithm', 'unknown')}, "
                      f"silhouette={col_result.get('silhouette', -1):.3f}")
    
    print("\nâœ… Question-wise classification pipeline test completed!")