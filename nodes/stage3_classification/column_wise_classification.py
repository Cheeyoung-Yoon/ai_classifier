"""
Column-wise Clustering Pipeline
ê° ì»¬ëŸ¼ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ê³  ì²« ë²ˆì§¸ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­í•˜ëŠ” íŒŒì´í”„ë¼ì¸
"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from typing import Dict, Any, Tuple, List, Optional, Union
import json
from pathlib import Path
from collections import defaultdict

class ColumnWiseClassification:
    """
    ì»¬ëŸ¼ë³„ í´ëŸ¬ìŠ¤í„°ë§ í›„ ë§¤ì¹­í•˜ëŠ” íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize column-wise classification pipeline
        
        Args:
            config: Configuration dictionary
        """
        self.config = config or self._get_default_config()
        
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration for column-wise clustering"""
        return {
            'algorithm': 'adaptive',  # adaptive, kmeans, dbscan, hierarchical
            'kmeans_k_range': [3, 4, 5, 6, 7],
            'dbscan_eps_range': [0.1, 0.2, 0.3, 0.4],
            'dbscan_min_samples_range': [3, 5, 7],
            'hierarchical_k_range': [3, 4, 5, 6, 7],
            'hierarchical_linkage': ['ward', 'complete', 'average'],
            'max_clusters': 10,
            'min_samples_per_column': 10,  # ì»¬ëŸ¼ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            'selection_criteria': 'silhouette',
            'matching_strategy': 'majority_vote',  # majority_vote, weighted_similarity
            'reference_column_index': 0  # ê¸°ì¤€ ì»¬ëŸ¼ ì¸ë±ìŠ¤
        }
    
    def cluster_by_columns(self, embeddings_dict: Dict[str, np.ndarray], 
                          metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê° ì»¬ëŸ¼ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        
        Args:
            embeddings_dict: ì»¬ëŸ¼ë³„ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬ {column_name: embeddings}
            metadata: ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì»¬ëŸ¼ë³„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
        """
        print(f"ğŸ”§ Column-wise clustering for {len(embeddings_dict)} columns...")
        
        column_results = {}
        column_names = list(embeddings_dict.keys())
        
        # ê° ì»¬ëŸ¼ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        for i, (col_name, embeddings) in enumerate(embeddings_dict.items()):
            print(f"\nğŸ“Š Processing column: {col_name} ({len(embeddings)} samples)")
            
            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
            if len(embeddings) < self.config['min_samples_per_column']:
                print(f"âš ï¸  Skipping {col_name}: insufficient samples ({len(embeddings)} < {self.config['min_samples_per_column']})")
                continue
            
            # ê°œë³„ ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§
            result = self._cluster_single_column(embeddings, col_name)
            if result is not None:
                column_results[col_name] = result
        
        # ê¸°ì¤€ ì»¬ëŸ¼ í™•ì¸
        reference_column = column_names[self.config['reference_column_index']]
        if reference_column not in column_results:
            print(f"âš ï¸  Reference column {reference_column} not found, using first available")
            reference_column = list(column_results.keys())[0] if column_results else None
        
        if not reference_column:
            raise ValueError("No valid clustering results found")
        
        print(f"ğŸ¯ Using reference column: {reference_column}")
        
        # ë§¤ì¹­ ìˆ˜í–‰
        matched_results = self._match_clusters_to_reference(
            column_results, reference_column, metadata
        )
        
        return {
            'column_results': column_results,
            'reference_column': reference_column,
            'matched_results': matched_results,
            'total_columns': len(embeddings_dict),
            'processed_columns': len(column_results)
        }
    
    def _cluster_single_column(self, embeddings: np.ndarray, 
                              column_name: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì»¬ëŸ¼ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            from optimized_classification import OptimizedClassification
            
            # ê°œë³„ ì»¬ëŸ¼ìš© ì„¤ì •
            column_config = self.config.copy()
            column_config['max_clusters'] = min(self.config['max_clusters'], len(embeddings) // 3)
            
            classifier = OptimizedClassification(column_config)
            result = classifier.cluster_embeddings(embeddings)
            
            if result is not None:
                result['column_name'] = column_name
                result['sample_count'] = len(embeddings)
                
                print(f"   âœ… {column_name}: {result['n_clusters']} clusters, "
                      f"silhouette={result.get('silhouette', -1):.3f}")
                
                return result
            
        except Exception as e:
            print(f"   âŒ Error clustering {column_name}: {e}")
            
        return None
    
    def _match_clusters_to_reference(self, column_results: Dict[str, Dict], 
                                   reference_column: str,
                                   metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê¸°ì¤€ ì»¬ëŸ¼ì˜ í´ëŸ¬ìŠ¤í„°ì— ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë§¤ì¹­
        """
        print(f"\nğŸ”— Matching clusters to reference: {reference_column}")
        
        reference_result = column_results[reference_column]
        reference_labels = reference_result['labels']
        reference_clusters = set(reference_labels)
        
        print(f"   Reference has {len(reference_clusters)} clusters")
        
        # ê° í–‰(row)ë³„ í´ëŸ¬ìŠ¤í„° ì •ë³´ ìˆ˜ì§‘
        row_mappings = self._build_row_mappings(metadata)
        
        # ê¸°ì¤€ ì»¬ëŸ¼ì˜ ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì˜ ì •ë³´ ë§¤ì¹­
        cluster_profiles = {}
        
        for ref_cluster in reference_clusters:
            if ref_cluster == -1:  # ë…¸ì´ì¦ˆ í´ëŸ¬ìŠ¤í„° ìŠ¤í‚µ
                continue
                
            cluster_profiles[ref_cluster] = self._build_cluster_profile(
                ref_cluster, reference_labels, column_results, row_mappings
            )
        
        # ìµœì¢… í†µí•© ë¼ë²¨ ìƒì„±
        final_labels = self._generate_final_labels(
            reference_labels, cluster_profiles, column_results
        )
        
        return {
            'final_labels': final_labels,
            'cluster_profiles': cluster_profiles,
            'reference_clusters': len(reference_clusters),
            'matching_strategy': self.config['matching_strategy'],
            'row_mappings': row_mappings
        }
    
    def _build_row_mappings(self, metadata: Dict[str, Any]) -> Dict[int, Dict[str, int]]:
        """
        ê° í–‰ì´ ì–´ë–¤ ì»¬ëŸ¼ì˜ ëª‡ ë²ˆì§¸ ì„ë² ë”©ì— í•´ë‹¹í•˜ëŠ”ì§€ ë§¤í•‘ êµ¬ì„±
        """
        row_mappings = {}
        
        # metadataì—ì„œ í–‰-ì»¬ëŸ¼ ë§¤í•‘ ì •ë³´ ì¶”ì¶œ
        if 'row_mapping' in metadata:
            for global_idx, (original_row, col_idx) in enumerate(metadata['row_mapping']):
                if original_row not in row_mappings:
                    row_mappings[original_row] = {}
                
                # ì»¬ëŸ¼ëª… ë§¤í•‘
                col_name = metadata.get('col_mapping', {}).get(col_idx, f"col_{col_idx}")
                row_mappings[original_row][col_name] = global_idx
        
        return row_mappings
    
    def _build_cluster_profile(self, ref_cluster: int, reference_labels: np.ndarray,
                              column_results: Dict[str, Dict],
                              row_mappings: Dict[int, Dict[str, int]]) -> Dict[str, Any]:
        """
        ê¸°ì¤€ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ì˜ í´ëŸ¬ìŠ¤í„° ë¶„í¬ í”„ë¡œíŒŒì¼ êµ¬ì„±
        """
        profile = {
            'reference_cluster': ref_cluster,
            'size': np.sum(reference_labels == ref_cluster),
            'column_distributions': {},
            'majority_clusters': {},
            'confidence_scores': {}
        }
        
        # ê¸°ì¤€ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤ì˜ ì¸ë±ìŠ¤
        ref_indices = np.where(reference_labels == ref_cluster)[0]
        
        # ê° ì»¬ëŸ¼ë³„ë¡œ í•´ë‹¹ ìƒ˜í”Œë“¤ì˜ í´ëŸ¬ìŠ¤í„° ë¶„í¬ ê³„ì‚°
        for col_name, col_result in column_results.items():
            if col_name not in row_mappings or len(row_mappings[col_name]) == 0:
                continue
                
            col_labels = col_result['labels']
            
            # ê¸°ì¤€ í´ëŸ¬ìŠ¤í„° ìƒ˜í”Œë“¤ì´ ì´ ì»¬ëŸ¼ì—ì„œ ì–´ë–¤ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            mapped_clusters = []
            
            for ref_idx in ref_indices:
                # row_mappingsë¥¼ í†µí•´ í•´ë‹¹ ìƒ˜í”Œì´ ì´ ì»¬ëŸ¼ì˜ ëª‡ ë²ˆì§¸ ì„ë² ë”©ì— í•´ë‹¹í•˜ëŠ”ì§€ ì°¾ê¸°
                # ê°„ë‹¨í™”: ì¸ë±ìŠ¤ ê¸°ë°˜ ë§¤í•‘ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë§¤í•‘ í•„ìš”)
                if ref_idx < len(col_labels):
                    mapped_clusters.append(col_labels[ref_idx])
            
            if mapped_clusters:
                # í´ëŸ¬ìŠ¤í„° ë¶„í¬ ê³„ì‚°
                unique_clusters, counts = np.unique(mapped_clusters, return_counts=True)
                distribution = dict(zip(unique_clusters.astype(int), counts.astype(int)))
                
                # ê°€ì¥ ë§ì€ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                majority_cluster = unique_clusters[np.argmax(counts)]
                confidence = np.max(counts) / len(mapped_clusters)
                
                profile['column_distributions'][col_name] = distribution
                profile['majority_clusters'][col_name] = int(majority_cluster)
                profile['confidence_scores'][col_name] = float(confidence)
        
        return profile
    
    def _generate_final_labels(self, reference_labels: np.ndarray,
                              cluster_profiles: Dict[int, Dict],
                              column_results: Dict[str, Dict]) -> np.ndarray:
        """
        ìµœì¢… í†µí•© ë¼ë²¨ ìƒì„±
        """
        if self.config['matching_strategy'] == 'majority_vote':
            return self._majority_vote_labels(reference_labels, cluster_profiles)
        elif self.config['matching_strategy'] == 'weighted_similarity':
            return self._weighted_similarity_labels(reference_labels, cluster_profiles, column_results)
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œëŠ” reference ë¼ë²¨ ì‚¬ìš©
            return reference_labels
    
    def _majority_vote_labels(self, reference_labels: np.ndarray,
                             cluster_profiles: Dict[int, Dict]) -> np.ndarray:
        """
        ë‹¤ìˆ˜ê²° íˆ¬í‘œ ë°©ì‹ìœ¼ë¡œ ìµœì¢… ë¼ë²¨ ê²°ì •
        """
        # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ reference ë¼ë²¨ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” ê° ì»¬ëŸ¼ì˜ í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¢…í•©í•´ì„œ ê²°ì •
        return reference_labels
    
    def _weighted_similarity_labels(self, reference_labels: np.ndarray,
                                   cluster_profiles: Dict[int, Dict],
                                   column_results: Dict[str, Dict]) -> np.ndarray:
        """
        ê°€ì¤‘ ìœ ì‚¬ë„ ë°©ì‹ìœ¼ë¡œ ìµœì¢… ë¼ë²¨ ê²°ì •
        """
        # í˜„ì¬ëŠ” ë‹¨ìˆœíˆ reference ë¼ë²¨ ë°˜í™˜
        # ì‹¤ì œë¡œëŠ” ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ë“±ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
        return reference_labels
    
    def create_results_dataframe(self, results: Dict[str, Any],
                                embeddings_dict: Dict[str, np.ndarray]) -> pd.DataFrame:
        """
        ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        """
        data = []
        
        # ê¸°ì¤€ ì»¬ëŸ¼ ì •ë³´
        reference_column = results['reference_column']
        matched_results = results['matched_results']
        final_labels = matched_results['final_labels']
        
        for i, label in enumerate(final_labels):
            row_data = {
                'sample_index': i,
                'final_cluster': label,
                'reference_column': reference_column
            }
            
            # ê° ì»¬ëŸ¼ë³„ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
            for col_name, col_result in results['column_results'].items():
                if i < len(col_result['labels']):
                    row_data[f'{col_name}_cluster'] = col_result['labels'][i]
                    row_data[f'{col_name}_algorithm'] = col_result.get('selected_algorithm', 'unknown')
            
            data.append(row_data)
        
        return pd.DataFrame(data)


def create_column_wise_classification_pipeline(config: Optional[Dict] = None) -> ColumnWiseClassification:
    """Create column-wise classification pipeline"""
    return ColumnWiseClassification(config)


if __name__ == "__main__":
    # Test the column-wise pipeline
    import sys
    sys.path.insert(0, str(Path(__file__).parent))
    
    from data_loader import load_data_from_state
    
    print("ğŸ§ª Testing Column-wise Classification Pipeline")
    print("=" * 60)
    
    # Load test data
    state_file = "/home/cyyoon/test_area/ai_text_classification/2.langgraph/data/test/state_history/20250918_113231_081_STAGE2_WORD_ë¬¸23_COMPLETED_state.json"
    
    with open(state_file, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    embeddings, metadata = load_data_from_state(state)
    
    print(f"Total embeddings loaded: {embeddings.shape}")
    
    # ì»¬ëŸ¼ë³„ë¡œ ì„ë² ë”© ë¶„ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)
    # ì‹¤ì œë¡œëŠ” metadataì—ì„œ ì»¬ëŸ¼ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ í•¨
    n_columns = 5
    samples_per_column = len(embeddings) // n_columns
    
    embeddings_dict = {}
    for i in range(n_columns):
        start_idx = i * samples_per_column
        end_idx = min((i + 1) * samples_per_column, len(embeddings))
        embeddings_dict[f'column_{i+1}'] = embeddings[start_idx:end_idx]
    
    print(f"Split into {len(embeddings_dict)} columns:")
    for col_name, col_emb in embeddings_dict.items():
        print(f"  {col_name}: {col_emb.shape}")
    
    # Test column-wise clustering
    classifier = create_column_wise_classification_pipeline()
    results = classifier.cluster_by_columns(embeddings_dict, metadata)
    
    print(f"\nğŸ¯ Column-wise Clustering Results:")
    print(f"   Processed columns: {results['processed_columns']}/{results['total_columns']}")
    print(f"   Reference column: {results['reference_column']}")
    
    # Show results for each column
    for col_name, col_result in results['column_results'].items():
        print(f"   {col_name}: {col_result['n_clusters']} clusters, "
              f"algorithm={col_result.get('selected_algorithm', 'unknown')}")
    
    # Create results dataframe
    results_df = classifier.create_results_dataframe(results, embeddings_dict)
    print(f"\nğŸ“Š Results DataFrame: {results_df.shape}")
    print(results_df.head())
    
    print("\nâœ… Column-wise classification pipeline test completed!")