#!/usr/bin/env python3
"""
ì™„ì „í•œ ê°œë³„ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ - ëª¨ë“  ë¬¸ì œì  í•´ê²°
"""

import os
import sys
import time
import tempfile
import shutil
import traceback
import pandas as pd
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any, List, Callable, Tuple
import json
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class ComprehensiveLogger:
    """í¬ê´„ì ì¸ ë¡œê·¸ ì¶œë ¥ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.indent_level = 0
        self.test_start_time = None
        self.node_start_time = None
        self.detailed_logs = []
        
    def log(self, message: str, level: str = "INFO"):
        """ë ˆë²¨ë³„ ë¡œê·¸ ì¶œë ¥"""
        indent = "  " * self.indent_level
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        
        # ë¡œê·¸ ê¸°ë¡
        self.detailed_logs.append({
            'timestamp': timestamp,
            'level': level,
            'message': message,
            'indent': self.indent_level
        })
        
        if level == "HEADER":
            print(f"\n{'='*80}")
            print(f"{indent}ğŸ¯ {message}")
            print(f"{'='*80}")
        elif level == "NODE":
            print(f"\n{'-'*60}")
            print(f"{indent}ğŸ”§ {message}")
            print(f"{'-'*60}")
        elif level == "TEST":
            print(f"\n{indent}ğŸ§ª {message}")
        elif level == "SUCCESS":
            print(f"{indent}âœ… {message}")
        elif level == "ERROR":
            print(f"{indent}âŒ {message}")
        elif level == "WARNING":
            print(f"{indent}âš ï¸  {message}")
        elif level == "DEBUG":
            print(f"{indent}ğŸ” [{timestamp}] {message}")
        elif level == "INFO":
            print(f"{indent}â„¹ï¸  {message}")
        elif level == "TIMER":
            print(f"{indent}â±ï¸  {message}")
        elif level == "ANALYSIS":
            print(f"{indent}ğŸ“Š {message}")
        else:
            print(f"{indent}{message}")
    
    def start_timer(self, context: str = "test"):
        """íƒ€ì´ë¨¸ ì‹œì‘"""
        if context == "node":
            self.node_start_time = time.time()
        else:
            self.test_start_time = time.time()
    
    def end_timer(self, context: str = "test") -> float:
        """íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ê²½ê³¼ ì‹œê°„ ë°˜í™˜"""
        if context == "node" and self.node_start_time:
            elapsed = time.time() - self.node_start_time
            self.log(f"Node execution time: {elapsed:.3f}s", "TIMER")
            return elapsed
        elif self.test_start_time:
            elapsed = time.time() - self.test_start_time
            self.log(f"Test execution time: {elapsed:.3f}s", "TIMER")
            return elapsed
        return 0.0
    
    def indent(self):
        """ì¸ë´íŠ¸ ì¦ê°€"""
        self.indent_level += 1
    
    def dedent(self):
        """ì¸ë´íŠ¸ ê°ì†Œ"""
        self.indent_level = max(0, self.indent_level - 1)
    
    def get_summary_stats(self) -> Dict[str, int]:
        """ë¡œê·¸ í†µê³„ ìƒì„±"""
        stats = {}
        for log_entry in self.detailed_logs:
            level = log_entry['level']
            stats[level] = stats.get(level, 0) + 1
        return stats


class ComprehensiveNodeTester:
    """í¬ê´„ì ì¸ ë…¸ë“œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.logger = ComprehensiveLogger()
        self.temp_dir = None
        self.project_dir = None
        self.test_results = {}
        self.node_execution_times = {}
        self.detailed_test_results = {}
        
    def setup_complete_test_environment(self):
        """ì™„ì „í•œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        self.logger.log("ì™„ì „í•œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...", "INFO")
        self.temp_dir = tempfile.mkdtemp()
        
        # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self.project_dir = Path(project_root) / "data" / "complete_node_test"
        self.project_dir.mkdir(parents=True, exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        survey_content = """
Q1. ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?
â‘  ë§¤ìš° ë§Œì¡± â‘¡ ë§Œì¡± â‘¢ ë³´í†µ â‘£ ë¶ˆë§Œì¡± â‘¤ ë§¤ìš° ë¶ˆë§Œì¡±

Q2. ì œí’ˆ í’ˆì§ˆì— ëŒ€í•œ ì˜ê²¬ì„ ìì„¸íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
(ììœ  ì„œìˆ í˜•)

Q3. ê°€ê²© ëŒ€ë¹„ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?
â‘  ë§¤ìš° ì¢‹ìŒ â‘¡ ì¢‹ìŒ â‘¢ ë³´í†µ â‘£ ë‚˜ì¨ â‘¤ ë§¤ìš° ë‚˜ì¨

Q4. ì¶”ì²œí•˜ê³  ì‹¶ì€ ì •ë„ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.
(ììœ  ì‘ë‹µ)

Q5. ì„œë¹„ìŠ¤ í’ˆì§ˆì€ ì–´ë– ì…¨ë‚˜ìš”?
(ììœ  ì˜ê²¬)
        """
        
        # DataFrame with proper ID column
        df = pd.DataFrame({
            'ID': [1, 2, 3, 4, 5, 6, 7, 8],
            'Q1': ['ë§¤ìš° ë§Œì¡±', 'ë§Œì¡±', 'ë³´í†µ', 'ë¶ˆë§Œì¡±', 'ë§¤ìš° ë§Œì¡±', 'ë§Œì¡±', 'ë³´í†µ', 'ë§¤ìš° ë§Œì¡±'],
            'Q2': [
                'í’ˆì§ˆì´ ì •ë§ ì¢‹ì•„ìš”. ê¸°ëŒ€ ì´ìƒì…ë‹ˆë‹¤.',
                'ê´œì°®ì€ í¸ì´ì§€ë§Œ ê°œì„ í•  ì ì´ ìˆì–´ìš”.',
                'ê·¸ëƒ¥ ë³´í†µì´ì—ìš”. íŠ¹ë³„í•˜ì§€ ì•Šì•„ìš”.',
                'í’ˆì§ˆì´ ì•„ì‰¬ì›Œìš”. ë” ì¢‹ì•„ì¡Œìœ¼ë©´ í•´ìš”.',
                'í’ˆì§ˆ ìµœê³ ! ë§¤ìš° ë§Œì¡±í•©ë‹ˆë‹¤.',
                'ì¢‹ì€ í’ˆì§ˆì´ì§€ë§Œ ê°€ê²©ì´ ë¹„ì‹¸ìš”.',
                'í’ˆì§ˆì€ ë³´í†µì¸ë° ë””ìì¸ì´ ë§ˆìŒì— ë“¤ì–´ìš”.',
                'ë§¤ìš° ìš°ìˆ˜í•œ í’ˆì§ˆì…ë‹ˆë‹¤. ê°•ë ¥ ì¶”ì²œ!'
            ],
            'Q3': ['ì¢‹ìŒ', 'ë³´í†µ', 'ë‚˜ì¨', 'ë³´í†µ', 'ë§¤ìš° ì¢‹ìŒ', 'ì¢‹ìŒ', 'ë‚˜ì¨', 'ë§¤ìš° ì¢‹ìŒ'],
            'Q4': [
                'ì ê·¹ ì¶”ì²œí•˜ê³  ì‹¶ì–´ìš”!',
                'ì§€ì¸ì—ê²Œ ì¶”ì²œí•  ì˜í–¥ ìˆì–´ìš”.',
                'ì¶”ì²œí•˜ì§€ ì•Šì„ ê²ƒ ê°™ì•„ìš”.',
                'ì˜ ëª¨ë¥´ê² ì–´ìš”.',
                'ê¼­ ì¶”ì²œí•˜ê³  ì‹¶ì–´ìš”!',
                'ê°€ê²©ë§Œ ì €ë ´í•˜ë©´ ì¶”ì²œí•˜ê² ì–´ìš”.',
                'íŠ¹ë³„íˆ ì¶”ì²œí•˜ì§€ëŠ” ì•Šì„ ê²ƒ ê°™ì•„ìš”.',
                'ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤!'
            ],
            'Q5': [
                'ì„œë¹„ìŠ¤ê°€ ë§¤ìš° ì¹œì ˆí•´ìš”.',
                'ì„œë¹„ìŠ¤ëŠ” ë³´í†µì´ì—ìš”.',
                'ì„œë¹„ìŠ¤ê°€ ì¢€ ì•„ì‰¬ì›Œìš”.',
                'ì„œë¹„ìŠ¤ í’ˆì§ˆì´ ë³„ë¡œì˜ˆìš”.',
                'ì„œë¹„ìŠ¤ê°€ ì •ë§ ì¢‹ì•„ìš”!',
                'ì„œë¹„ìŠ¤ëŠ” ë‚˜ì˜ì§€ ì•Šì•„ìš”.',
                'ì„œë¹„ìŠ¤ ê°œì„ ì´ í•„ìš”í•´ìš”.',
                'ìµœê³ ì˜ ì„œë¹„ìŠ¤!'
            ]
        })
        
        # íŒŒì¼ ì €ì¥
        survey_file = Path(self.temp_dir) / "comprehensive_survey.txt"
        data_file = Path(self.temp_dir) / "comprehensive_data.xlsx"
        
        survey_file.write_text(survey_content, encoding='utf-8')
        df.to_excel(data_file, index=False, engine='openpyxl')
        
        self.logger.log(f"ì„ì‹œ ë””ë ‰í† ë¦¬: {self.temp_dir}", "DEBUG")
        self.logger.log(f"í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬: {self.project_dir}", "DEBUG")
        self.logger.log(f"ì„¤ë¬¸ íŒŒì¼: {survey_file} ({len(survey_content)} chars)", "DEBUG")
        self.logger.log(f"ë°ì´í„° íŒŒì¼: {data_file} {df.shape}", "DEBUG")
        
        return str(survey_file), str(data_file)
    
    def cleanup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬"""
        if self.temp_dir:
            shutil.rmtree(self.temp_dir, ignore_errors=True)
            self.logger.log("ì„ì‹œ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ", "INFO")
        
        if self.project_dir:
            shutil.rmtree(self.project_dir, ignore_errors=True)
            self.logger.log("í”„ë¡œì íŠ¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ", "INFO")
    
    def safe_compare_values(self, old_val, new_val, key: str) -> bool:
        """DataFrame ë“±ì„ ì•ˆì „í•˜ê²Œ ë¹„êµ"""
        try:
            if isinstance(old_val, pd.DataFrame) and isinstance(new_val, pd.DataFrame):
                return not old_val.equals(new_val)
            elif isinstance(old_val, pd.DataFrame) or isinstance(new_val, pd.DataFrame):
                return True  # í•˜ë‚˜ë§Œ DataFrameì¸ ê²½ìš° ë³€ê²½ë¨
            else:
                return old_val != new_val
        except (ValueError, TypeError):
            return True
    
    def test_single_node_comprehensive(self, node_name: str, node_function: Callable, 
                                     input_state: Dict[str, Any], 
                                     test_description: str = "",
                                     expected_outputs: List[str] = None) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ ë‹¨ì¼ ë…¸ë“œ í…ŒìŠ¤íŠ¸"""
        self.logger.log(f"Testing Node: {node_name}", "NODE")
        if test_description:
            self.logger.log(f"Description: {test_description}", "INFO")
        
        self.logger.indent()
        self.logger.start_timer("node")
        
        test_result = {
            'node_name': node_name,
            'success': False,
            'execution_time': 0.0,
            'input_state_size': len(input_state),
            'output_state_size': 0,
            'new_fields': [],
            'modified_fields': [],
            'error_message': '',
            'validation_results': {},
            'result_state': {}
        }
        
        try:
            # ì…ë ¥ ìƒíƒœ ë¡œê¹…
            self.logger.log("Input State Analysis:", "DEBUG")
            self.logger.indent()
            for key, value in input_state.items():
                if isinstance(value, (dict, list)):
                    self.logger.log(f"{key}: {type(value).__name__} (length: {len(value)})", "DEBUG")
                elif isinstance(value, pd.DataFrame):
                    self.logger.log(f"{key}: DataFrame {value.shape}", "DEBUG")
                elif value is None:
                    self.logger.log(f"{key}: None", "DEBUG")
                else:
                    value_str = str(value)[:50]
                    self.logger.log(f"{key}: {value_str}{'...' if len(str(value)) > 50 else ''}", "DEBUG")
            self.logger.dedent()
            
            # ë…¸ë“œ ì‹¤í–‰
            self.logger.log("Executing node function...", "INFO")
            result_state = node_function(input_state.copy())
            
            # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            execution_time = self.logger.end_timer("node")
            test_result['execution_time'] = execution_time
            self.node_execution_times[node_name] = execution_time
            
            # ê²°ê³¼ ìƒíƒœ ë¶„ì„
            self.logger.log("Output State Analysis:", "DEBUG")
            self.logger.indent()
            
            if isinstance(result_state, dict):
                test_result['output_state_size'] = len(result_state)
                test_result['result_state'] = result_state
                
                for key, value in result_state.items():
                    if key not in input_state:
                        test_result['new_fields'].append(key)
                        if isinstance(value, (dict, list)):
                            self.logger.log(f"NEW {key}: {type(value).__name__} (length: {len(value)})", "DEBUG")
                        elif isinstance(value, pd.DataFrame):
                            self.logger.log(f"NEW {key}: DataFrame {value.shape}", "DEBUG")
                        else:
                            value_str = str(value)[:50]
                            self.logger.log(f"NEW {key}: {value_str}{'...' if len(str(value)) > 50 else ''}", "DEBUG")
                    elif self.safe_compare_values(input_state[key], value, key):
                        test_result['modified_fields'].append(key)
                        if isinstance(value, (dict, list)):
                            self.logger.log(f"MODIFIED {key}: {type(value).__name__} (length: {len(value)})", "DEBUG")
                        elif isinstance(value, pd.DataFrame):
                            self.logger.log(f"MODIFIED {key}: DataFrame {value.shape}", "DEBUG")
                        else:
                            value_str = str(value)[:50]
                            self.logger.log(f"MODIFIED {key}: {value_str}{'...' if len(str(value)) > 50 else ''}", "DEBUG")
            
            self.logger.dedent()
            
            # ê¸°ë³¸ ê²€ì¦
            validations = {}
            
            if not isinstance(result_state, dict):
                validations['return_type'] = f"âŒ Must return dict, got {type(result_state)}"
                raise AssertionError(f"Node must return dict, got {type(result_state)}")
            else:
                validations['return_type'] = "âœ… Returns dict"
            
            if 'current_stage' not in result_state:
                validations['current_stage'] = "âš ï¸ Missing 'current_stage'"
                self.logger.log("Warning: No 'current_stage' in result", "WARNING")
            else:
                validations['current_stage'] = f"âœ… Has current_stage: {result_state['current_stage']}"
            
            if 'error' in result_state:
                validations['error_handling'] = f"âš ï¸ Error present: {result_state['error']}"
                self.logger.log(f"Node completed with error: {result_state['error']}", "WARNING")
            else:
                validations['error_handling'] = "âœ… No errors"
            
            # ì˜ˆìƒ ì¶œë ¥ ê²€ì¦
            if expected_outputs:
                for expected_output in expected_outputs:
                    if expected_output in result_state:
                        validations[f'expected_{expected_output}'] = f"âœ… Has {expected_output}"
                    else:
                        validations[f'expected_{expected_output}'] = f"âŒ Missing {expected_output}"
            
            test_result['validation_results'] = validations
            test_result['success'] = True
            
            self.logger.log(f"Node {node_name} executed successfully!", "SUCCESS")
            self.logger.log(f"New fields: {test_result['new_fields']}", "ANALYSIS")
            self.logger.log(f"Modified fields: {test_result['modified_fields']}", "ANALYSIS")
            
            self.logger.dedent()
            return test_result
            
        except Exception as e:
            test_result['error_message'] = str(e)
            self.logger.log(f"Node {node_name} failed: {str(e)}", "ERROR")
            self.logger.indent()
            self.logger.log("Traceback:", "DEBUG")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self.logger.log(line, "DEBUG")
            self.logger.dedent()
            self.logger.dedent()
            
            return test_result


def comprehensive_test_all_nodes():
    """ëª¨ë“  ë…¸ë“œì˜ í¬ê´„ì  í…ŒìŠ¤íŠ¸"""
    tester = ComprehensiveNodeTester()
    
    # í…ŒìŠ¤íŠ¸ ì‹œì‘
    tester.logger.log("í¬ê´„ì ì¸ ê°œë³„ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘", "HEADER")
    tester.logger.start_timer()
    
    try:
        # Mock setup
        class MockProjectManager:
            def __init__(self, project_name: str = "test", base_dir: str = "/tmp"):
                self.project_name = project_name
                self.base_dir = base_dir
                self.state_file_path = "/tmp/mock_state.json"
            
            def save_state(self, state, config):
                pass
            
            def create_project_structure(self):
                return {"project_dir": "/tmp/mock_project"}
        
        with patch('utils.project_manager.ProjectDirectoryManager', MockProjectManager):
            
            # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
            survey_file_path, data_file_path = tester.setup_complete_test_environment()
            
            # ê¸°ë³¸ ìƒíƒœ ìƒì„±
            base_state = {
                'project_name': 'complete_node_test',
                'survey_file_path': survey_file_path,
                'data_file_path': data_file_path,
                'pipeline_id': 'complete_test_pipeline_001',
                'current_stage': 'INITIALIZATION',
                'total_llm_cost_usd': 0.0,
                'questions': {},
                'data': None,
                'survey_raw_content': '',
                'survey_context': '',
                'question_data_match': {},
                'open_columns': [],
                'question_processing_queue': [],
                'current_question_idx': 0,
                'current_question': None,
                'current_data_sample': [],
                'stage2_processing_complete': False,
                'raw_dataframe_path': data_file_path
            }
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
            all_test_results = []
            
            # =================================================================
            # STAGE 1 COMPREHENSIVE TESTS
            # =================================================================
            tester.logger.log("STAGE 1 ë…¸ë“œë“¤ í¬ê´„ì  í…ŒìŠ¤íŠ¸", "HEADER")
            
            # 1. Survey Loader Test
            from nodes.stage1_data_preparation.survey_loader import load_survey_node
            result = tester.test_single_node_comprehensive(
                "survey_loader",
                load_survey_node,
                base_state.copy(),
                "ì„¤ë¬¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì›ì‹œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œ",
                expected_outputs=['raw_survey_info']
            )
            all_test_results.append(result)
            
            # Survey loader ê²°ê³¼ë¥¼ ë‹¤ìŒ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©
            survey_state = base_state.copy()
            if result['success'] and 'raw_survey_info' in result['result_state']:
                survey_state.update(result['result_state'])
            
            # 2. Data Loader Test
            from nodes.stage1_data_preparation.data_loader import load_data_node
            result = tester.test_single_node_comprehensive(
                "data_loader",
                load_data_node,
                survey_state.copy(),
                "Excel ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  DataFrameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë…¸ë“œ",
                expected_outputs=['raw_data_info', 'raw_dataframe_path']
            )
            all_test_results.append(result)
            
            # Data loader ê²°ê³¼ë¥¼ ë‹¤ìŒ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©
            data_state = survey_state.copy()
            if result['success']:
                data_state.update(result['result_state'])
            
            # 3. Survey Parser Test (with mocking)
            from nodes.stage1_data_preparation.survey_parser import parse_survey_node
            
            with patch('io_layer.llm.client.LLMClient') as mock_llm_client, \
                 patch('config.prompt.prompt_loader.resolve_branch') as mock_resolve_branch:
                
                # Mock ì„¤ì •
                mock_response = {
                    "raw": "Survey parsed successfully",
                    "parsed": [
                        {
                            "question_number": "Q1",
                            "question_text": "ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?",
                            "question_type": "MULTIPLE_CHOICE",
                            "choices": {"1": "ë§¤ìš° ë§Œì¡±", "2": "ë§Œì¡±", "3": "ë³´í†µ", "4": "ë¶ˆë§Œì¡±", "5": "ë§¤ìš° ë¶ˆë§Œì¡±"}
                        },
                        {
                            "question_number": "Q2", 
                            "question_text": "ì œí’ˆ í’ˆì§ˆì— ëŒ€í•œ ì˜ê²¬ì„ ìì„¸íˆ ë§ì”€í•´ì£¼ì„¸ìš”.",
                            "question_type": "OPEN_ENDED",
                            "choices": {}
                        },
                        {
                            "question_number": "Q4", 
                            "question_text": "ì¶”ì²œí•˜ê³  ì‹¶ì€ ì •ë„ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.",
                            "question_type": "OPEN_ENDED",
                            "choices": {}
                        },
                        {
                            "question_number": "Q5", 
                            "question_text": "ì„œë¹„ìŠ¤ í’ˆì§ˆì€ ì–´ë– ì…¨ë‚˜ìš”?",
                            "question_type": "OPEN_ENDED",
                            "choices": {}
                        }
                    ]
                }
                
                mock_llm_instance = Mock()
                mock_llm_instance.chat.return_value = (mock_response, Mock())
                mock_llm_client.return_value = mock_llm_instance
                
                mock_resolve_branch.return_value = {
                    'system': 'You are a survey parser',
                    'user_template': 'Parse this survey: {survey_content}',
                    'schema': Mock()
                }
                
                result = tester.test_single_node_comprehensive(
                    "survey_parser",
                    parse_survey_node,
                    data_state.copy(),
                    "LLMì„ ì‚¬ìš©í•˜ì—¬ ì„¤ë¬¸ì„ íŒŒì‹±í•˜ê³  ì§ˆë¬¸ êµ¬ì¡°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œ",
                    expected_outputs=['parsed_survey', 'llm_logs', 'llm_meta']
                )
                all_test_results.append(result)
            
            # =================================================================
            # ROUTER TESTS (COMPREHENSIVE)
            # =================================================================
            tester.logger.log("ë¼ìš°í„°ë“¤ í¬ê´„ì  í…ŒìŠ¤íŠ¸", "HEADER")
            
            # Stage2 Type Router Tests (fixed)
            from router.stage2_router import stage2_type_router
            
            tester.logger.log("Stage2 Type Router í…ŒìŠ¤íŠ¸", "NODE")
            
            router_test_cases = [
                # ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
                ({'current_question': {'question_type': 'WORD', 'question_id': 'Q1'}}, 'WORD'),
                ({'current_question': {'question_type': 'SENTENCE', 'question_id': 'Q2'}}, 'SENTENCE'),
                ({'current_question': {'question_type': 'ETC', 'question_id': 'Q3'}}, 'ETC'),
                ({'current_question': None}, '__END__'),
                # íŠ¹ë³„ ì¼€ì´ìŠ¤: question_id ì¶”ê°€ë¡œ ë¼ìš°í„° ë™ì‘ í™•ì¸
                ({
                    'current_question': {'question_type': 'WORD', 'question_id': 'Q1'},
                    'current_question_id': 'Q1'
                }, 'WORD'),
                ({
                    'current_question': {'question_type': 'SENTENCE', 'question_id': 'Q2'},
                    'current_question_id': 'Q2'
                }, 'SENTENCE'),
            ]
            
            router_successes = 0
            for i, (input_state, expected) in enumerate(router_test_cases):
                try:
                    tester.logger.log(f"Router test case {i+1}: {input_state.get('current_question', 'None')}", "TEST")
                    result = stage2_type_router(input_state)
                    
                    if result == expected:
                        tester.logger.log(f"âœ… Expected: {expected}, Got: {result}", "SUCCESS")
                        router_successes += 1
                    else:
                        tester.logger.log(f"âŒ Expected: {expected}, Got: {result}", "ERROR")
                        
                except Exception as e:
                    tester.logger.log(f"âŒ Router test {i+1} failed: {e}", "ERROR")
            
            router_success_rate = router_successes / len(router_test_cases)
            tester.logger.log(f"Router Tests: {router_successes}/{len(router_test_cases)} passed ({router_success_rate*100:.1f}%)", 
                            "SUCCESS" if router_success_rate >= 0.8 else "WARNING")
            
            # =================================================================
            # FINAL COMPREHENSIVE SUMMARY
            # =================================================================
            total_time = tester.logger.end_timer()
            
            tester.logger.log("í¬ê´„ì  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„", "HEADER")
            
            # ì„±ê³µí•œ ë…¸ë“œ ë¶„ì„
            successful_nodes = [result for result in all_test_results if result['success']]
            failed_nodes = [result for result in all_test_results if not result['success']]
            
            tester.logger.log(f"ì„±ê³µí•œ ë…¸ë“œ: {len(successful_nodes)}", "SUCCESS")
            tester.logger.log(f"ì‹¤íŒ¨í•œ ë…¸ë“œ: {len(failed_nodes)}", "ERROR")
            
            # ê°œë³„ ë…¸ë“œ ë¶„ì„
            tester.logger.log("ê°œë³„ ë…¸ë“œ ìƒì„¸ ë¶„ì„:", "ANALYSIS")
            tester.logger.indent()
            
            for result in all_test_results:
                node_name = result['node_name']
                success = "âœ…" if result['success'] else "âŒ"
                exec_time = result['execution_time']
                new_fields = len(result['new_fields'])
                modified_fields = len(result['modified_fields'])
                
                tester.logger.log(f"{success} {node_name}: {exec_time:.3f}s, "
                                f"{new_fields} new, {modified_fields} modified", "ANALYSIS")
                
                # ê²€ì¦ ê²°ê³¼ í‘œì‹œ
                if result['validation_results']:
                    tester.logger.indent()
                    for validation, result_text in result['validation_results'].items():
                        tester.logger.log(f"{validation}: {result_text}", "DEBUG")
                    tester.logger.dedent()
            
            tester.logger.dedent()
            
            # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
            tester.logger.log("ì‹¤í–‰ ì‹œê°„ ë¶„ì„:", "TIMER")
            tester.logger.indent()
            for node_name, exec_time in tester.node_execution_times.items():
                if exec_time > 1.0:
                    tester.logger.log(f"ğŸŒ {node_name}: {exec_time:.3f}s (ëŠë¦¼)", "WARNING")
                elif exec_time > 0.1:
                    tester.logger.log(f"â±ï¸ {node_name}: {exec_time:.3f}s (ë³´í†µ)", "INFO")
                else:
                    tester.logger.log(f"âš¡ {node_name}: {exec_time:.3f}s (ë¹ ë¦„)", "SUCCESS")
            tester.logger.dedent()
            
            # ë¡œê·¸ í†µê³„
            log_stats = tester.logger.get_summary_stats()
            tester.logger.log("ë¡œê·¸ í†µê³„:", "ANALYSIS")
            tester.logger.indent()
            for level, count in log_stats.items():
                tester.logger.log(f"{level}: {count}", "DEBUG")
            tester.logger.dedent()
            
            # ìµœì¢… ìš”ì•½
            success_rate = len(successful_nodes) / len(all_test_results) if all_test_results else 0
            tester.logger.log("ğŸ¯ ìµœì¢… í¬ê´„ì  ìš”ì•½", "HEADER")
            tester.logger.log(f"ì´ í…ŒìŠ¤íŠ¸ëœ ë…¸ë“œ: {len(all_test_results)}", "INFO")
            tester.logger.log(f"ì„±ê³µë¥ : {success_rate*100:.1f}%", "SUCCESS" if success_rate >= 0.8 else "WARNING")
            tester.logger.log(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ", "TIMER")
            tester.logger.log(f"í‰ê·  ë…¸ë“œ ì‹¤í–‰ ì‹œê°„: {sum(tester.node_execution_times.values())/len(tester.node_execution_times):.3f}ì´ˆ", "TIMER")
            
            if success_rate >= 0.8:
                tester.logger.log("ğŸ‰ ëŒ€ë¶€ë¶„ì˜ ë…¸ë“œê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!", "SUCCESS")
            else:
                tester.logger.log("âš ï¸ ì¼ë¶€ ë…¸ë“œì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê°œë³„ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.", "WARNING")
            
            # ê°œì„  ì‚¬í•­ ìš”ì•½
            tester.logger.log("í…ŒìŠ¤íŠ¸ ê°œì„  ì‚¬í•­:", "INFO")
            tester.logger.indent()
            tester.logger.log("â€¢ í¬ê´„ì ì¸ ìƒíƒœ ë¶„ì„ ë° ë¡œê¹…", "INFO")
            tester.logger.log("â€¢ ì•ˆì „í•œ DataFrame ë¹„êµ", "INFO")
            tester.logger.log("â€¢ ì‹¤í–‰ ì‹œê°„ ì„±ëŠ¥ ë¶„ì„", "INFO")
            tester.logger.log("â€¢ ìƒì„¸í•œ ê²€ì¦ ê²°ê³¼", "INFO")
            tester.logger.log("â€¢ Mock ê¸°ë°˜ ì˜ì¡´ì„± ê²©ë¦¬", "INFO")
            tester.logger.dedent()
            
            return success_rate >= 0.8
            
    except Exception as e:
        tester.logger.log(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
        tester.logger.indent()
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                tester.logger.log(line, "DEBUG")
        tester.logger.dedent()
        return False
        
    finally:
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬
        tester.cleanup_test_environment()


if __name__ == "__main__":
    success = comprehensive_test_all_nodes()
    sys.exit(0 if success else 1)