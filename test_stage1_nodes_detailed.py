#!/usr/bin/env python3
"""
Individual Node Test Suite - Stage 1 Data Preparation Nodes
Stage 1 ë°ì´í„° ì¤€ë¹„ ë…¸ë“œë“¤ì˜ ê°œë³„ ì„¸ë¶€ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import tempfile
import shutil
import pandas as pd
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Import Stage 1 nodes
from nodes.stage1_data_preparation.survey_loader import load_survey_node
from nodes.stage1_data_preparation.data_loader import load_data_node
from nodes.stage1_data_preparation.survey_parser import parse_survey_node
from nodes.stage1_data_preparation.survey_context import survey_context_node
from nodes.stage1_data_preparation.column_extractor import get_open_column_node
from nodes.stage1_data_preparation.question_matcher import question_data_matcher_node
from nodes.stage1_data_preparation.memory_optimizer import stage1_memory_flush_node


class TestSurveyLoaderNode:
    """Detailed tests for survey loader node"""
    
    def setup_method(self):
        self.temp_dir = tempfile.mkdtemp()
        self.survey_file = Path(self.temp_dir) / "test_survey.txt"
    
    def teardown_method(self):
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_load_valid_survey_file(self):
        """Test loading a valid survey file"""
        print("ğŸ§ª Testing valid survey file loading")
        
        # Create test survey content
        survey_content = """
        Q1. ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?
        â‘  ë§¤ìš° ë§Œì¡± â‘¡ ë§Œì¡± â‘¢ ë³´í†µ â‘£ ë¶ˆë§Œì¡± â‘¤ ë§¤ìš° ë¶ˆë§Œì¡±
        
        Q2. ì¶”ì²œí•˜ê³  ì‹¶ì€ ì •ë„ëŠ”?
        (ììœ  ì„œìˆ )
        """
        self.survey_file.write_text(survey_content, encoding='utf-8')
        
        state = {
            'survey_file_path': str(self.survey_file),
            'current_stage': 'INITIALIZATION'
        }
        
        result = load_survey_node(state)
        
        # Assertions
        assert 'survey_raw_content' in result
        assert len(result['survey_raw_content']) > 0
        assert 'Q1' in result['survey_raw_content']
        assert 'Q2' in result['survey_raw_content']
        assert result['current_stage'] == 'STAGE1_SURVEY_LOADED'
        assert 'ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„' in result['survey_raw_content']
        
        print("âœ… Valid survey file loading passed")
    
    def test_load_nonexistent_file(self):
        """Test loading non-existent survey file"""
        print("ğŸ§ª Testing non-existent file handling")
        
        state = {
            'survey_file_path': '/nonexistent/path/survey.txt',
            'current_stage': 'INITIALIZATION'
        }
        
        try:
            result = load_survey_node(state)
            assert False, "Should have raised an exception"
        except FileNotFoundError:
            print("âœ… Correctly handled non-existent file")
        except Exception as e:
            print(f"âœ… Exception handled: {e}")
    
    def test_load_empty_survey_file(self):
        """Test loading empty survey file"""
        print("ğŸ§ª Testing empty survey file")
        
        self.survey_file.write_text("", encoding='utf-8')
        
        state = {
            'survey_file_path': str(self.survey_file),
            'current_stage': 'INITIALIZATION'
        }
        
        result = load_survey_node(state)
        
        assert 'survey_raw_content' in result
        assert result['survey_raw_content'] == ""
        assert result['current_stage'] == 'STAGE1_SURVEY_LOADED'
        
        print("âœ… Empty survey file handling passed")


class TestDataLoaderNode:
    """Detailed tests for data loader node"""
    
    def setup_method(self):
        self.temp_dir = tempfile.mkdtemp()
        self.csv_file = Path(self.temp_dir) / "test_data.csv"
        self.xlsx_file = Path(self.temp_dir) / "test_data.xlsx"
    
    def teardown_method(self):
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_load_csv_file(self):
        """Test loading CSV data file"""
        print("ğŸ§ª Testing CSV file loading")
        
        # Create test CSV data
        test_data = pd.DataFrame({
            'Q1': ['ë§¤ìš° ë§Œì¡±', 'ë§Œì¡±', 'ë³´í†µ', 'ë¶ˆë§Œì¡±'],
            'Q2': ['ì¶”ì²œí•¨', 'ë³´í†µ', 'ì¶”ì²œ ì•ˆí•¨', 'ëª¨ë¥´ê² ìŒ'],
            'Q3_ììœ ì‘ë‹µ': ['í’ˆì§ˆì´ ì¢‹ì•„ìš”', 'ê°€ê²©ì´ ì ë‹¹í•´ìš”', 'ì„œë¹„ìŠ¤ê°€ ì•„ì‰¬ì›Œìš”', 'ì „ë°˜ì ìœ¼ë¡œ ë§Œì¡±'],
            'respondent_id': [1, 2, 3, 4]
        })
        test_data.to_csv(self.csv_file, index=False, encoding='utf-8')
        
        state = {
            'data_file_path': str(self.csv_file),
            'current_stage': 'STAGE1_SURVEY_LOADED'
        }
        
        result = load_data_node(state)
        
        # Assertions
        assert 'data' in result
        assert result['data'] is not None
        assert isinstance(result['data'], pd.DataFrame)
        assert len(result['data']) == 4
        assert 'Q1' in result['data'].columns
        assert 'Q2' in result['data'].columns
        assert 'Q3_ììœ ì‘ë‹µ' in result['data'].columns
        assert result['current_stage'] == 'STAGE1_DATA_LOADED'
        
        print("âœ… CSV file loading passed")
    
    def test_load_xlsx_file(self):
        """Test loading Excel data file"""
        print("ğŸ§ª Testing Excel file loading")
        
        # Create test Excel data
        test_data = pd.DataFrame({
            'Q1': ['ë§¤ìš° ë§Œì¡±', 'ë§Œì¡±'],
            'Q2_ê°œë°©í˜•': ['ì •ë§ ì¢‹ì•„ìš”', 'ê´œì°®ìŠµë‹ˆë‹¤'],
            'ID': [101, 102]
        })
        test_data.to_excel(self.xlsx_file, index=False)
        
        state = {
            'data_file_path': str(self.xlsx_file),
            'current_stage': 'STAGE1_SURVEY_LOADED'
        }
        
        result = load_data_node(state)
        
        # Assertions
        assert 'data' in result
        assert isinstance(result['data'], pd.DataFrame)
        assert len(result['data']) == 2
        assert 'Q1' in result['data'].columns
        assert 'Q2_ê°œë°©í˜•' in result['data'].columns
        assert result['current_stage'] == 'STAGE1_DATA_LOADED'
        
        print("âœ… Excel file loading passed")
    
    def test_load_invalid_file_format(self):
        """Test loading invalid file format"""
        print("ğŸ§ª Testing invalid file format")
        
        # Create text file with invalid format
        invalid_file = Path(self.temp_dir) / "invalid.txt"
        invalid_file.write_text("This is not a valid data format", encoding='utf-8')
        
        state = {
            'data_file_path': str(invalid_file),
            'current_stage': 'STAGE1_SURVEY_LOADED'
        }
        
        try:
            result = load_data_node(state)
            # Should handle gracefully or raise appropriate exception
        except Exception as e:
            print(f"âœ… Correctly handled invalid format: {e}")


class TestSurveyParserNode:
    """Detailed tests for survey parser node"""
    
    @patch('nodes.stage1_data_preparation.survey_parser.resolve_branch')
    @patch('io_layer.llm.client.LLMClient')
    def test_parse_multiple_choice_questions(self, mock_llm_client, mock_resolve_branch):
        """Test parsing multiple choice questions"""
        print("ğŸ§ª Testing multiple choice question parsing")
        
        # Mock LLM response for multiple choice
        mock_response = {
            "raw": "Parsed questions",
            "parsed": [
                {
                    "question_number": "Q1",
                    "question_text": "ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?",
                    "question_type": "MULTIPLE_CHOICE",
                    "choices": {
                        "1": "ë§¤ìš° ë§Œì¡±",
                        "2": "ë§Œì¡±",
                        "3": "ë³´í†µ",
                        "4": "ë¶ˆë§Œì¡±",
                        "5": "ë§¤ìš° ë¶ˆë§Œì¡±"
                    }
                }
            ]
        }
        
        mock_llm_instance = Mock()
        mock_llm_instance.chat.return_value = (mock_response, Mock())
        mock_llm_client.return_value = mock_llm_instance
        
        mock_resolve_branch.return_value = {
            'system': 'You are a survey parser',
            'user_template': 'Parse: {survey_content}',
            'schema': Mock()
        }
        
        survey_content = """
        Q1. ë¸Œëœë“œì— ëŒ€í•œ ì „ë°˜ì ì¸ ë§Œì¡±ë„ëŠ” ì–´ë– ì‹ ê°€ìš”?
        â‘  ë§¤ìš° ë§Œì¡± â‘¡ ë§Œì¡± â‘¢ ë³´í†µ â‘£ ë¶ˆë§Œì¡± â‘¤ ë§¤ìš° ë¶ˆë§Œì¡±
        """
        
        state = {
            'survey_raw_content': survey_content,
            'current_stage': 'STAGE1_SURVEY_LOADED'
        }
        
        result = parse_survey_node(state)
        
        # Assertions
        assert 'questions' in result
        assert 'Q1' in result['questions']
        assert result['questions']['Q1']['question_type'] == 'MULTIPLE_CHOICE'
        assert 'choices' in result['questions']['Q1']
        assert result['current_stage'] == 'STAGE1_SURVEY_PARSED'
        
        print("âœ… Multiple choice question parsing passed")
    
    @patch('nodes.stage1_data_preparation.survey_parser.resolve_branch')
    @patch('io_layer.llm.client.LLMClient')
    def test_parse_open_ended_questions(self, mock_llm_client, mock_resolve_branch):
        """Test parsing open-ended questions"""
        print("ğŸ§ª Testing open-ended question parsing")
        
        # Mock LLM response for open-ended
        mock_response = {
            "raw": "Parsed questions",
            "parsed": [
                {
                    "question_number": "Q2",
                    "question_text": "ë¸Œëœë“œì— ëŒ€í•œ ì˜ê²¬ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.",
                    "question_type": "OPEN_ENDED",
                    "choices": {}
                }
            ]
        }
        
        mock_llm_instance = Mock()
        mock_llm_instance.chat.return_value = (mock_response, Mock())
        mock_llm_client.return_value = mock_llm_instance
        
        mock_resolve_branch.return_value = {
            'system': 'You are a survey parser',
            'user_template': 'Parse: {survey_content}',
            'schema': Mock()
        }
        
        survey_content = """
        Q2. ë¸Œëœë“œì— ëŒ€í•œ ì˜ê²¬ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.
        (ììœ  ì„œìˆ í˜•)
        """
        
        state = {
            'survey_raw_content': survey_content,
            'current_stage': 'STAGE1_SURVEY_LOADED'
        }
        
        result = parse_survey_node(state)
        
        # Assertions
        assert 'questions' in result
        assert 'Q2' in result['questions']
        assert result['questions']['Q2']['question_type'] == 'OPEN_ENDED'
        assert result['current_stage'] == 'STAGE1_SURVEY_PARSED'
        
        print("âœ… Open-ended question parsing passed")


class TestColumnExtractorNode:
    """Detailed tests for column extractor node"""
    
    def test_extract_open_columns_mixed_types(self):
        """Test extracting open columns from mixed question types"""
        print("ğŸ§ª Testing open column extraction with mixed types")
        
        # Setup test data with mixed question types
        test_data = pd.DataFrame({
            'Q1_ê°ê´€ì‹': ['ë§¤ìš° ë§Œì¡±', 'ë§Œì¡±', 'ë³´í†µ'],
            'Q2_ì£¼ê´€ì‹': ['í’ˆì§ˆì´ ì¢‹ì•„ìš”', 'ì„œë¹„ìŠ¤ê°€ ì•„ì‰¬ì›Œìš”', 'ê°€ê²©ì´ ì ë‹¹í•´ìš”'],
            'Q3_ì²™ë„': [5, 4, 3],
            'Q4_ììœ ì‘ë‹µ': ['ì¶”ì²œí•©ë‹ˆë‹¤', 'ë³´í†µì…ë‹ˆë‹¤', 'ê°œì„ ì´ í•„ìš”í•´ìš”'],
            'Q5_ì„ íƒí˜•': ['A', 'B', 'C']
        })
        
        questions = {
            'Q1': {'question_type': 'MULTIPLE_CHOICE', 'question_text': 'ë§Œì¡±ë„'},
            'Q2': {'question_type': 'OPEN_ENDED', 'question_text': 'ì˜ê²¬'},
            'Q3': {'question_type': 'SCALE', 'question_text': 'ì ìˆ˜'},
            'Q4': {'question_type': 'OPEN_ENDED', 'question_text': 'ììœ  ì˜ê²¬'},
            'Q5': {'question_type': 'MULTIPLE_CHOICE', 'question_text': 'ì„ íƒ'}
        }
        
        state = {
            'data': test_data,
            'questions': questions,
            'current_stage': 'STAGE1_SURVEY_PARSED'
        }
        
        result = get_open_column_node(state)
        
        # Assertions
        assert 'open_columns' in result
        open_columns = result['open_columns']
        
        # Should include open-ended questions
        assert 'Q2' in [col for col in open_columns if 'Q2' in col]
        assert 'Q4' in [col for col in open_columns if 'Q4' in col]
        
        # Should exclude multiple choice and scale questions
        q1_columns = [col for col in open_columns if 'Q1' in col]
        q3_columns = [col for col in open_columns if 'Q3' in col]
        q5_columns = [col for col in open_columns if 'Q5' in col]
        
        assert len(q1_columns) == 0  # Multiple choice excluded
        assert len(q5_columns) == 0  # Multiple choice excluded
        
        assert result['current_stage'] == 'STAGE1_COLUMNS_EXTRACTED'
        
        print("âœ… Mixed type column extraction passed")
    
    def test_extract_no_open_columns(self):
        """Test extraction when no open columns exist"""
        print("ğŸ§ª Testing extraction with no open columns")
        
        test_data = pd.DataFrame({
            'Q1': ['A', 'B', 'C'],
            'Q2': [1, 2, 3],
            'Q3': ['ì„ íƒ1', 'ì„ íƒ2', 'ì„ íƒ3']
        })
        
        questions = {
            'Q1': {'question_type': 'MULTIPLE_CHOICE'},
            'Q2': {'question_type': 'SCALE'},
            'Q3': {'question_type': 'MULTIPLE_CHOICE'}
        }
        
        state = {
            'data': test_data,
            'questions': questions,
            'current_stage': 'STAGE1_SURVEY_PARSED'
        }
        
        result = get_open_column_node(state)
        
        # Assertions
        assert 'open_columns' in result
        assert len(result['open_columns']) == 0
        assert result['current_stage'] == 'STAGE1_COLUMNS_EXTRACTED'
        
        print("âœ… No open columns extraction passed")


def run_stage1_tests():
    """Run all Stage 1 node tests"""
    print("ğŸš€ Starting Stage 1 Node Detailed Tests")
    print("=" * 50)
    
    test_classes = [
        TestSurveyLoaderNode(),
        TestDataLoaderNode(),
        TestSurveyParserNode(),
        TestColumnExtractorNode()
    ]
    
    total_tests = 0
    passed_tests = 0
    failed_tests = []
    
    for test_class in test_classes:
        class_name = test_class.__class__.__name__
        print(f"\nğŸ“‹ Running {class_name}")
        print("-" * 30)
        
        test_methods = [method for method in dir(test_class) if method.startswith('test_')]
        
        for test_method in test_methods:
            total_tests += 1
            try:
                if hasattr(test_class, 'setup_method'):
                    test_class.setup_method()
                
                getattr(test_class, test_method)()
                passed_tests += 1
                
                if hasattr(test_class, 'teardown_method'):
                    test_class.teardown_method()
                    
            except Exception as e:
                failed_tests.append(f"{class_name}.{test_method}: {str(e)}")
                print(f"âŒ {test_method} failed: {e}")
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“Š STAGE 1 TEST SUMMARY")
    print("=" * 50)
    print(f"Total Tests: {total_tests}")
    print(f"Passed: {passed_tests}")
    print(f"Failed: {len(failed_tests)}")
    print(f"Success Rate: {(passed_tests/total_tests)*100:.1f}%")
    
    if failed_tests:
        print("\nâŒ FAILED TESTS:")
        for failure in failed_tests:
            print(f"  - {failure}")
    else:
        print("\nğŸ‰ ALL STAGE 1 TESTS PASSED!")


if __name__ == "__main__":
    run_stage1_tests()